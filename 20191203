1. 딥하게 네트워크 구조를 해야 하는 이유?
네트워크 계층 구조를 기억해 두고, 만약에 개를 인식하는 문제를 생각해보면,
layer가 한개라면 한번에 '이헤' 해야 하기 때문에
신경망을 깊게 한다면, 학습해야 할 문제를 계층적으로 분해할 수 있어. 
( CNN 계층에서는 에지 등의 단순한 패턴에 뉴런이 반응하고 층이 깊어지면서 질감과 같이 점차 복잡한 것에 반응한다. )
또 층을 깊게하면 정보를 계층적으로 전달할 수 있어
예, 에지를 추출한 다음 층은 에지 정보를 쓸수 있고 더 고도의 패턴을 효과적으로 학습하리라 기대할 수 있어 
즉, 층을 깊게 함으로써 각 층이 학습해야 할 문제를 '풀기 위한 단순한 문제'로 분해 할수 있어 효율적임

2. VGG 
합성곱 계층과 풀링 계층으로 구성되는 기본적인 CNN
네크워크 깊이를 깊게 만드는게 성능에 어떤 영향을 미치는지를 확인 하고자 함(vgg 연구원팀)
3X3 의 작은 필터를 사용한 합성곱 계층을 연속적으로 거친다.
(필터 사이즈가 크면 그만큼 이미지 사이즈가 금방 축소괴기 때문에 네트워크 깊이를 충분히 깊게 만들기 불가능하다)
'3*3 필터로 두차례 콘볼루션 하는 것 = 5*5 필터 한번 콘볼루션' 인거 알지?
3*3 필터 2번 콘볼루션 하는 장점은?
3*3 필터가 2개 이면 총 18게 가중치를 갖고, 5*5 필터 1개는 25개 가중치를 갖기 때문에 3*3 필터 2개가 가중치가 작아 학습 속도가 빨라지며 층의 갯수가 늘어나면서 특성에 비선형성을 더 증가 시키기 때문에 특성이 더 유용해짐

인풋 224*224*3
1층 conv1_1 : 64개 3*3*3 필터커널로 입력 이미지를 컨볼루션, ( zeropadding=1, stride=1 : 모든 층 동일)
1층 conv1_1 결과 : 64장의 224*224 특성맵 (224*224*64)
1층 conv1_1 활성화 : ReLu 함수 적용 (모든 층에서 적용, 마지막 층은 적용 안됌)
2층 conv1_2 : 64개 3*3*64 필터커널로 특성맵을 컨볼루션
2층 conv1_2 결과 : 64장의 224*224 특성맵 (224*224*64)
2층 conv1_2 풀링 : 2*2 최대풀링을 stride=2 적용하면 112*112*64 특성맵

3층 conv2_1 : 128개 3*3*64 필터커널로 특성맵을 컨볼루션
3층 conv2_1 결과 : 128장의 112*112 특성맵 (112*112*128)
4층 conv2_2 : 128개 3*3*128 필터커널로 특성맵을 컨볼루션
4층 conv2_2 결과 : 128장의 112*112 특성맵 (112*112*128) 
4층 conv2_2 풀링 : 2*2 최대풀링을 stride=2 적용하면 56*56*128 특성맵

5층 conv3_1 : 256개 3*3*128 필터커널로 특성맵을 컨볼루션
5층 conv3_1 결과 : 256장의 56*56 특성맵 (56*56*256)
6층 conv3_2 : 256개의 3*3*256 필터커널로 특성맵 컨볼루션
6층 conv3_2 결과 : 256장의 56*56 특성맵 (56*56*256)
7층 conv3_3 : 256개의 3*3*256 필터커널로 특성맵을 컨볼루션
7층 conv3_3 결과 : 256장의 56*56 특성맵 (56*56*256)
7층 conv3_3 풀링 : 2*2 최대풀링을 stride=2 적용하면 28*28*256 특성맵

8층 conv4_1 : 512개의 3*3*256 필터커널 특성맵을 컨볼루션
8층 conv4_1 결과 : 512개의 28*28*256 특성맵 (28*28*512)
9층 conv4_2 : 512개의 3*3*512 필터커널 특성맵을 컨볼루션
9층 conv4_2 결과 : 512개의 28*28*512 특성맵 (28*28*512)
10층 conv4_3 : 512개의 3*3*256 필터커널 특성맵을 컨볼루션
10층 conv4_3 풀링 : 2*2 최대풀링을 stride=2 적용하면 14*14*512 특성맵
10층 conv4_4 결과 : 512개의 14*14*512 특성맵 (14*14*512)

11층 conv5_1 : 512개의 3*3*256 필터커널 특성맵을 컨볼루션
11층 conv5_1 결과 : 512개의 14*14*512 특성맵 (14*14*512)
12층 conv5_2 : 512개의 3*3*512 필터커널 특성맵을 컨볼루션
12층 conv5_2 결과 : 512개의 14*14*512 특성맵 (14*14*512)

13층 conv5_3 : 512개의 3*3*512 필터커널 특성맵을 컨볼루션
13층 conv5_3 풀링 : 2*2 최대풀링 stride=2 적용하면 7*7*512
13층 conv5_3 결과 : 512개의 7*7*512 특성맵 (7*7*512)

14층 fully connect 1 : 4096개 7*7*512 필터커널 특성맵 컨볼루션
14층 fully connect 1 결과 : 4096개의 1*1 특성맵 (4096개의 뉴런)
15층 fully connect 2 : 4096개의 뉴런, fc1층의 4096개의 뉴런과 연결. 훈련시 dropout 적용
16층 fully connect 3 : 10000개의 뉴런, fc2층의 4096개의 뉴런과 연결
16층 fully connect 3 출력 : softmax 함수로 활성화, 1000개 클래스로 분류된 네트워크

3. LeNet
CNN 중에 조상격?, 최종버전 LeNet-5
LeNet은 CNN을 처음으로 개발한 얀르쿤(Yann Lecun)연구팀이 1998년에 개발함

C : Convolution layer, S : subsampling layer
convolution 에서 훈련해야할 파라미터 개수 : (가중치*입력맵개수 + 바이어스)*특성맵개수 
subsampling 에서 훈련해야할 파라미터 개수 : (입력맵개수 + 바이어스)*특성맵개수
fully connect 에서 훈련해야할 파라미터 개수 : (입력개수 + 바이어스)*출력개수 
subsampling 에서 평균 풀링인데 왜 훈련해야할 파라미터가 필요한가?
평균을 낸 후, 한개의 훈련 가능한 가중치를 곱해주고 또 한개의 훈련가능한 바이어스를 더해준다. 그 값들이 시그모이드 함수를 통해 활성화 된다.
그 가중치와 바이어스는 시그모이드의 비활성도를 조절해준다.

인풋 32*3
C1 : 입풋 - 6개의 5*5 필터 간의 컨볼루션 연산
C1 결과 : 6장의 28*28 특성맵, 훈련해야할 파라미터 개수= (5*5*1 +1)*6 =156

S2 : 6장의 28*28 특성맵 에 대해 서브 샘플링
S2 결과 : 14*14 특성맵, 줄어든 이유: 2*2 필터를 stride=2로 설정해서 서브샘플링 해주기 때문, 서브샘플링 방법은 average pooling, 훈련해야할 파라미터 개수: (1+1)*6=12

C3 : 6장의 14*14 특성맵에 컨볼루션 연산
C3 결과 : 16장의 10*10  ?? , 456+606+303+151 =총 1516
C3 결과 추가설명 : 
6장의 14*14 특성맵에서 연속된 3장씩 모아 5*5*3 필터와 컨볼루션 = 6장의 10*10 특성맵, (5*5*3+1)*6 =456
6장의 14*14 특성맵에서 연속됭 4장씩 모아 5*5*4 필터와 컨볼루션 = 6장의 10*10 특성맵, (5*5*4+1)*6 =606
6장의 14*14 특성맵에서 불연속된 4장씩 모아 5*5*4 필터와 컨볼루션 = 3장의 10*10 특성맵, (5*5*4+1)*3 =303
6장의 14*14 특성맵에서 모두를 가지고 5*5*6 필터와 컨볼루션 = 1장의 10*10 특성맵, (5*5*6+1)*1 =151

S4 : 16장의 10*10 특성맵 에 대해 서브 샘플링
S4 결과 : 16장의 5*5 특성맵, (1+1)*16 =32

C5 : 16장의 5*5 특성맵 - 120개 5*5*16 필터와 컨볼루션
C5 결과 : 120*1*1 특성맵, (5*5*16 +1)*120 =48120

FC6 : 84개의 유닛을 가진 feedforward network. 120*1*1 특성맵 - 84 유닛 간의 연결, 훈련해야할 파라미터 개수: (120+1)*84 =10164

아웃풋 10개의 euclidean radial basis function 유닛들로 구성됨

LeNet-5 를 제대로 가동하기 위해 훈련해야 할 파라미터 156+12+32+48120+10164 = 총60000


<참고자료>
[1] 밑바닥 부터 시작하는 딥러닝 8장 p267
[2] https://bskyvision.com/504, VGGNet의 구조(VGG16)
[3] https://bskyvision.com/418, Lenet-5의 구조
