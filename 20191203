1. 딥하게 네트워크 구조를 해야 하는 이유?
네트워크 계층 구조를 기억해 두고, 만약에 개를 인식하는 문제를 생각해보면,
layer가 한개라면 한번에 '이헤' 해야 하기 때문에
신경망을 깊게 한다면, 학습해야 할 문제를 계층적으로 분해할 수 있어. 
( CNN 계층에서는 에지 등의 단순한 패턴에 뉴런이 반응하고 층이 깊어지면서 질감과 같이 점차 복잡한 것에 반응한다. )
또 층을 깊게하면 정보를 계층적으로 전달할 수 있어
예, 에지를 추출한 다음 층은 에지 정보를 쓸수 있고 더 고도의 패턴을 효과적으로 학습하리라 기대할 수 있어 
즉, 층을 깊게 함으로써 각 층이 학습해야 할 문제를 '풀기 위한 단순한 문제'로 분해 할수 있어 효율적임

2. VGG 
합성곱 계층과 풀링 계층으로 구성되는 기본적인 CNN
네크워크 깊이를 깊게 만드는게 성능에 어떤 영향을 미치는지를 확인 하고자 함(vgg 연구원팀)
3X3 의 작은 필터를 사용한 합성곱 계층을 연속적으로 거친다.
(필터 사이즈가 크면 그만큼 이미지 사이즈가 금방 축소괴기 때문에 네트워크 깊이를 충분히 깊게 만들기 불가능하다)
'3*3 필터로 두차례 콘볼루션 하는 것 = 5*5 필터 한번 콘볼루션' 인거 알지?
3*3 필터 2번 콘볼루션 하는 장점은?
3*3 필터가 2개 이면 총 18게 가중치를 갖고, 5*5 필터 1개는 25개 가중치를 갖기 때문에 3*3 필터 2개가 가중치가 작아 학습 속도가 빨라지며 층의 갯수가 늘어나면서 특성에 비선형성을 더 증가 시키기 때문에 특성이 더 유용해짐

인풋 224*224*3
1층 conv1_1 : 64개 3*3*3 필터커널로 입력 이미지를 컨볼루션, ( zeropadding=1, stride=1 : 모든 층 동일)
1층 conv1_1 결과 : 64장의 224*224 특성맵 (224*224*64)
1층 conv1_1 활성화 : ReLu 함수 적용 (모든 층에서 적용, 마지막 층은 적용 안됌)
2층 conv1_2 : 64개 3*3*64 필터커널로 특성맵을 컨볼루션
2층 conv1_2 결과 : 64장의 224*224 특성맵 (224*224*64)
2층 conv1_2 풀링 : 2*2 최대풀링을 stride=2 적용하면 112*112*64 특성맵
2층 conv1_2 활성화 : ReLu 함수 적용

3층 conv2_1 : 128개 3*3*64 필터커널로 특성맵을 컨볼루션
3층 conv2_1 결과 : 128장의 112*112 특성맵 (112*112*128)
4층 conv2_2 : 128개 3*3*128 필터커널로 특성맵을 컨볼루션
4층 conv2_2 결과 : 128장의 112*112 특성맵 (112*112*128) 
4층 conv2_2 풀링 : 2*2 최대풀링을 stride=2 적용하면 56*56*128 특성맵

5층 conv3_1 : 256개 3*3*128 필터커널로 특성맵을 컨볼루션
5층 conv3_1 결과 : 256장의 56*56 특성맵 (56*56*256)
6층 conv3_2 : 256개의 3*3*256 필터커널로 특성맵 컨볼루션
6층 conv3_2 결과 : 256장의 56*56 특성맵 (56*56*256)
7층 conv3_3 : 256개의 3*3*256 필터커널로 특성맵을 컨볼루션
7층 conv3_3 결과 : 256장의 56*56 특성맵 (56*56*256)
7층 conv3_3 풀링 : 2*2 최대풀링을 stride=2 적용하면 28*28*256 특성맵

8층 conv4_1 : 512개의 3*3*256 필터커널 특성맵을 컨볼루션
8층 conv4_1 결과 : 512개의 28*28*256 특성맵 (28*28*512)

9층 conv4_2 : 512개의 3*3*256 필터커널 특성맵을 컨볼루션
9층 conv4_2 결과 : 512개의 28*28*512 특성맵 (28*28*512)
10층 conv4_3 : 512개의 3*3*256 필터커널 특성맵을 컨볼루션
10층 conv4_3 풀링 : 2*2 최대풀링을 stride=2 적용하면 14*14*512 특성맵
10층 conv4_4 결과 : 512개의 14*14*512 특성맵 (14*14*512)

11층 conv5_1 : 512개의 3*3*256 필터커널 특성맵을 컨볼루션
11층 conv5_1 결과 : 512개의 14*14*512 특성맵 (14*14*512)
12층 conv5_2 : 512개의 3*3*256 필터커널 특성맵을 컨볼루션
12층 conv5_2 결과 : 512개의 14*14*512 특성맵 (14*14*512)

13층 conv5_3 : 512개의 3*3*256 필터커널 특성맵을 컨볼루션
13층 conv5_3 풀링 : 2*2 최대풀링 stride=2 적용하면 7*7*512
13층 conv5_3 결과 : 512개의 7*7*512 특성맵 (7*7*512)

14층 fully connect 1 : 4096개 7*7*512 필터커널 특성맵 컨볼루션
14층 fully connect 1 결과 : 4096개의 1*1 특성맵 (4096개의 뉴런)
15층 fully connect 2 : 4096개의 뉴런, fc1층의 4096개의 뉴런과 연결. 훈련시 dropout 적용
16층 fully connect 3 : 10000개의 뉴런, fc2층의 4096개의 뉴런과 연결
16층 fully connect 3 출력 : softmax 함수로 활성화, 1000개 클래스로 분류된 네트워크

<참고자료>
[1] 밑바닥 부터 시작하는 딥러닝 8장 p267
[2] https://bskyvision.com/504, VGGNet의 구조(VGG16)
