# visualization
종류
- visualize patches that maximally activate neurons
- visualize the weights
- visualize the representation space(ex: with t-SNE)
- occlusion experiments
- huma experiment comparisons
- deconv approaches (single backward pass)
- optimization over image approaches (optimization)
  
**conv 역할이 무엇인지 알기 위해서는 conv layer에서 raw activation 을 봐야함
예를 들어 conv layer 에서 224x224x64 이라면 224x224 이 activations 임** 
  
### cnn에서 가장 activations 시키는 것이 어떤 뉴런인지 확인하여 visualization 
![image](https://user-images.githubusercontent.com/56099627/71167722-8c279880-2298-11ea-94af-e56eb5d8950c.png)
- 가장 activations 시키는 뉴런이 어떤 것인지 시각화 해서 보여줌
- pool5 레이어에 임의의 뉴런을 선택한 다음에 여러장의 이미지들을 convnet을 돌려준다. pool5에서 추출한 임의의 뉴런을 가장 activations 시키는 것이 어떤 것인지 살펴본다 (파란선: 강아지 또는 닷 형태에 반응하는 행)
### kernel을 visaulization (raw weights)
![image](https://user-images.githubusercontent.com/56099627/71168253-9bf3ac80-2299-11ea-9aab-a8b40067f04e.png)  
- 위 그림은 gobor filter 인데 이것은 특정 방향성에 대한 외곽선을 검출하는 텍스쳐 분석에 사용하는 필터
- conv1 에 대한 filters은 그림과 같이 나옴. **이미지에 직접 작용하는 필터는 conv1 에 있는 필터만 임** 그래서 1번째 레이어만 해석 가능함. conv1 layer 이후의 레이어 들은 visualization을 할 수는 있지만 이전 레이어의 weight에 대한 visualizaiton 이기 때문에 해석이 용이하지 않다?
### representatiion을 visaulization 하는 방법
![image](https://user-images.githubusercontent.com/56099627/71168513-3b18a400-229a-11ea-903d-be02e15970c8.png)  
- FC7에서 4096차원의 코드들이 들어 있음. 마지막단 softmax 취해 classification을 하기 전 fully cnnected layer에서 visualization 하는 것임 (여러개의 이미지들의 코드를 모아 한번에 이것을 visualization 하는 것임 )
![image](https://user-images.githubusercontent.com/56099627/71168733-b5492880-229a-11ea-84c8-e2bf3b9f09ac.png)  
- 대표적인 방법으로 t-SNE visualization 은 cnn 시각으로 볼때 유사한 것들까리 가까운데로 위치시켜서 clustering 하는 것. 그림은 mnist 데이터를 사용하여 clustering 해준거임 (mnist은 28x28 이미지 라서 784-dimension vectors =28x28)
### occlusion experiments
<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/71151569-650c9f00-2277-11ea-9070-cec555aaab52.png" /></p> 
- 은닉을 통해 실험한것인데 인풋이미지에 일부를 가려서(정사각형으로 은폐한 부분) 해당 객체를 인식할 수 있는지 보았을때 현저히 인식이 떨어지는 것을 확인 할 수 있음. **은폐한 위치에** 따라서 객체를 얼마나 잘 classify 하는지(얼마나 그 확률에 얼마나 변화가 있는지) 히트맵을 통해 확인하는 실험을 함
- 그림을 보면 파란색일 수록 인식 능력이 떨어지는데 포메리안 강아지, 자동차 휠, 아프간 강아지 인식 능력이 떨이짐을 확인 할 수 있음

### Deconvolution-based approach-based approach
![image](https://user-images.githubusercontent.com/56099627/71154811-be78cc00-227f-11ea-8c5d-22e874e944e5.png)  
2종류 backpropagation, guided backpropagation 있음
  
![image](https://user-images.githubusercontent.com/56099627/71157010-8031db80-2284-11ea-97d8-e71fca66c9f6.png)  
그림에서 보면 그냥 backpropagation은 모든 뉴런에 0을 부여하고 내가 보고싶어하는 뉴런에만 1을 부여하여 backpropagation을 해주면 이미지에 대한 gradient을 시각화 하여 보여준다. 근데 결과를 보면 애매한 이미지 시각화가 나온다. (+)와 (-)가 시각화 하기 위해 싸우다가? 캔슬링 아웃되어 이렇게 애매한 이미지가 보여진것이다. 그래서 이것을 좀더 선명하게 보기 위해 **guided backpropagation방법** 을 사용한다. (+)인플루언스만 backpropagation 시에 반영하는 방법이다. 그래서 특정 feature에 해당하는 뉴런은 일반 backpropagation 보다 더 선명하게 시각화하여 볼수 있다.
  
![image](https://user-images.githubusercontent.com/56099627/71157651-c3407e80-2285-11ea-8404-fc3cb887547b.png)  
(그림) deconvnet은 activation으로 relu을 쓰지 않음(relu의 영향을 받지 않음) 그래서 backward 할 때 0보다 작은 값만 0으로 전달 되고 양수 값들은 그대로 전댈된다.


### Optimizationi-based approach
Deconvolution-based approach 보다 어렵지가 직관적인 이해는 쉬울 것임
  
![image](https://user-images.githubusercontent.com/56099627/71158971-364af480-2288-11ea-91da-12e77f4e4c41.png)  
이미지가 Optimization되는 파라미터가 된다
**일반적인 conv 에선 weight 들이 파라미터가 되었는데 여기에선 conv 에선 weight 들을 fixing 해주고 이미지를 파라미터로 이용한다.즉 이미지를 업데이트한다.** 그래서 특정 class을 score화 할수 있는 이미지를 찾는데 
  
<p align="center"><img width="40%" src="https://user-images.githubusercontent.com/56099627/71158769-da806b80-2287-11ea-95e9-a216b394b8d5.png" /></p> 
(그림) 특정 class의 score가 최대가 되는 값에 regularization(예: L2)을 빼줌
순서
- zero image을 forwardpassward을 해주고 
- 마지막단에서 우리가 관심있는 class의 score vector의 gradient을 1이라 설정해주고(나머진 다 0으로 변경) backpropagation을 해줌
- 결과적으로 이미지에 대한 업데이트를 약간 수행해주게 됨
- 방금 업데이트한 이미지를 네트워크에 forwardpass을 해준다(2번째 과정부터 다시 반복)

최초에는 제로 이미지로 시작해서 클래스가 어떤것을 보고 activation이 되는지를 확인 하면 된다. (class의 score 위치에서 최대한 maximize 하려 노력하는 구나 확인됨)

### visualize the Date gradient 
3개의 채널이 필요하고 3개의 채널에 대한 히트맵 어떠한 결과가 나오는지
grabcut 사용으로 segmetation - 실제론 잘 나오는지 잘.. 모르겠다함(의견) 
FC7 보다 앞단에서 하기 때문에 보다 location 이미지를 정확히 잡을수 있는 것임

![image](https://user-images.githubusercontent.com/56099627/71162185-1a4a5180-228e-11ea-95c3-71ba69920f8f.png)  
뒷단으로 갈수록 이미지 reconstructions가 blur 되는것을 확인 할 수있으며 앞쪽 단일수록 더 선명한 reconstruction 됨

goole DeepDream은 이미지에 대한 optimization 하는 방법임
이미지 업데이트 할때마다 make_step 함수를 호출
gradient을 activation값으로 설정하는게 핵심이고 그 activation이 relu이기 때문에 boosting 효과가 일어나서 합성하는듯한 결과가 나타난다.

### NeuralStyle
이미지를 optimization 하는 과정에서 만들어진 모델
1번 이미지(파카소)의 style과 2번 이미지(간달프?)의 모양(이미지 내용물)가 합성되어 만들어짐
'프리즈마' 라는 앱이 이 neural style 을 적용해주는 어플임

동작방법
- step 1) content target 이미지를 convnet에 넣고 각각의 layer에서 target activations을 저장해준다 (예: conv5에서 14x14x512을 저장해줌)
- step 2)style target 이미지의 스타일과 관련된 통계?는 raw activations에 있지 않고 pairwise statistics에 있음을 발견함. 특히 pairwise statistics 중의 style gram matrices 에 있음 (예: conv1에서 64차원의 224x224 activations 가 있는데 모든 공간에 대해 outer product을 해주고 이것들을 합쳐주면 64x64 gram matrix 생성하게 됨)
  - <img src="https://latex.codecogs.com/gif.latex?G=V^{^{T}}V" title="G=V^{^{T}}V" /></a> : 즉 이건 activation(224x224x64 )과 activation transpose(64x224x224)를 곱해주면 64x64 gram matrix 생성
  - gram matrix은 기본적으로 공분산 matrix 특성을 가짐 (두개의 feature가 곱으로 )
  
![image](https://user-images.githubusercontent.com/56099627/71165772-cb53ea80-2294-11ea-9d68-66ff8ba929b3.png)  
- step 3) image를 어떻게 optimize 해나가냐면.. 
  - content image (activations)
  - style image ( gram matrices of activations)
  ![image](https://user-images.githubusercontent.com/56099627/71165649-95166b00-2294-11ea-96a4-862ad339422e.png)
  - 각각의 content image loss, style image loss 가 존재하게 되고
  - 적절한 초기값으로 시작하는 이미지를 conv를 넣었을때 conten 쪽과 gram matrices 쪽이 매치하면서 loss을 최소화해 가는데 이 두개의 loss가 최소화해가면서 싸우게? 되면서 neuralstyle을 만들어감








참고  
[1] http://cs231n.stanford.edu/2016/syllabus.html, (설명) Andu song  
[2] https://www.youtube.com/watch?v=j_BeROoelLo, cs231n 9강 Visualizations, Adversarial examples
