Q: 만약에 activation func을 사용하지 않고 그냥 layers을 진행 할 경우? 
단일의 linear 한 func으로 표현 가능해짐
y=ax 라는 linear 함수가 있다면, 1번째 층에선 y=ax이고, 2번째 층에선 y=a(ax), 3번째 층에선 y=a(a(ax)),.. 이런식 일 것임
그래서 은닉계층이 없는 단일 계층을 사용하는 것의 결과가 나타날 것임(즉, linear classification 이 됨)

# Parameter update
### stochastic Gradient 은 왜 느릴까?
![image](https://user-images.githubusercontent.com/56099627/70980180-39ae7680-20f6-11ea-8fd0-a7b11cf981d7.png)  
(그림) loss func 이 수직으로는 경사가 급함, 반면 수평으로는 경사가 완만함. 그래서 수렴하기까지 오랜 시간이 걸려. 이런 문제를 해결하기 위해서 **momentum**을 적용함  
- SGG은 lr에 매우 민감한 편임. 특히 데이터가 sparse이며 features가 서로다른 frequencies (빈도수)를 갖고 있다면, 단 하나의 learning rate가 모든 weight update에 영향을 주는것은 문제가 될 수 있다.


### momentum update
![image](https://user-images.githubusercontent.com/56099627/70980894-73cc4800-20f7-11ea-92c2-1475d65f410c.png)  
- 기존: learning rate 에 바로 x의 위치(gradient)을 업데이트 해줌
- momentum 적용: v(velocity)을 먼저 구한 후, x의 위치(gradient)을 업데이트 해줌
  - mu (뮤): 마찰계수 이며, 일반적으로 0.5~0.99 범위로 사용함(때론, 0.5 에서 0.99 로 mu 값을 증가시켜 사용하기도 함)
  - 경사가 완만할 땐 x의 진행을 천천히, x의 진행을 빠르게 하여 수렴(convergence) 하게 해줌

![image](https://user-images.githubusercontent.com/56099627/70984913-a62d7380-20fe-11ea-8aaf-447acb3a357c.png)
  
(그림) SGD, momentum, nesterov 의 비교

### nesterov Momentum update
![image](https://user-images.githubusercontent.com/56099627/70982968-4ed9d400-20fb-11ea-860a-a6a719521f9d.png)  
  
(그림) momentum 와 nesterov Momentum 비교 : **모멤텀 스텝의 종료점에서 시작** 하여 gradient step을 진행함  
- 파라미터 벡터와 그 위치에서 gradient 을 구하게 되는데 nesterov에선 세타와 다른 위치에서의 gradient을 요구한다. 그러면 일반적인 api 코드와 호환성이 떨어진다. 

![image](https://user-images.githubusercontent.com/56099627/70984362-b2fd9780-20fd-11ea-85db-d5679730cce4.png)  

(그림) nesterov Momentum을 보기 좋은 형태로 ~ 업데이트 할 수 있는 수식 

### AdaGrad update
![image](https://user-images.githubusercontent.com/56099627/70986386-2359e800-2101-11ea-8419-7579f7dae6f7.png)  
- 파라미터 별로 다른 learning rate을 제공해주는 개념 : 파라미터들이 동일한 러닝레이트를 제공받는게 아니라 cache가 계속 building  되어가므로 파라미터 들이 각각 다른 러닝레이트 영향을 받게 된다 (**per- parameter adaptive learning rate method**)
  - (장점) momentum에선 weight 세타에 대해 **동일한 learning rate**을 사용하여 uptate을 하였는데 AdaGra 에선 각가 데이터에 대해 dynamically adapt 함으로서 **각 featrues 마다 서로 다른 lr을 연산**한다.
- 1e-7은 0으로 나누는 일을 방지하기 위한 역할
- (수직) grdient 가 크다는 것은 그만큼 csche 값이 커진다. 그래서 update 되는 곳의 분모에 또한 cache 값이 존재 하므로 분모의 값이 커지고 그만큼 update 진행 느리다. (수평) 경사가 완만하다. grdient 가 작다는 것은 그만큼 csche 값이 작아지고 그래서 update 되는 곳의 분모에 또한 cache 값이 존재 하므로 분모의 값이 작아지므로 그만큼 update 진행 빠르다. 
  - 경사에 경도?? 되지 않는 효과를 가져다 준다?
- 문제점: 스텝 사이즈가 시간에 흐름에 따라서 어떻게 되냐면, cache 값은 시간이 지남에 따라 계속 **building up** 되고 그렇게 진행되다 보면 learning rate 값은 0에 가까운 값이 될것임. 결국 학습이 종료됨
- 학습이 종료되지 않게 해주는 어떤 에너지를 제공해주는데 **RMSprop** 임
- convex problem 에서 시작된 AdaGrad은 convex optimization 에선 잘 작동 하지만 여러개의 레이어로 구성된 NN에서는 다소 적합하지 않음






참고  
[1] http://cs231n.stanford.edu/2016/syllabus.html, (설명) Andu song  
[2] https://www.youtube.com/watch?v=5t1E3LZ3FDY, cs231n 6강 Training NN part 2  
[3] http://incredible.ai/artificial-intelligence/2017/04/09/Optimizer-Adagrad/, [Optimizer] Adagrad
