### Q: 왜 반드시 activation func을 이용해야 하느냐?
NN에서 activation func이 없으면, NN 전체가 몇개의 레이어를 가지든지 간에 단일의 linear 한 func으로 표현 가능해짐  
y=ax 라는 linear 함수가 있다면, 1번째 층에선 y=ax이고, 2번째 층에선 y=a(ax), 3번째 층에선 y=a(a(ax)),.. 이런식 일 것임  
그래서 은닉계층이 없는 단일 계층을 사용하는 것의 결과가 나타날 것임(즉, linear classification 이 됨)  
  
# Parameter update
1st order optimization method
### stochastic Gradient 
![image](https://user-images.githubusercontent.com/56099627/70998252-35945000-211a-11ea-8ac5-edad3490c1e4.png)  
(그림) 일반 NN의 training에서 SGD  
  
**SGD은 왜 느릴까?**  
![image](https://user-images.githubusercontent.com/56099627/70980180-39ae7680-20f6-11ea-8fd0-a7b11cf981d7.png)  
(그림) loss func 이 수직으로는 경사가 급함, 반면 수평으로는 경사가 완만함. 그래서 SGD으로 수렴하기까지 오랜 시간이 걸려. 이런 문제를 해결하기 위해서 **momentum**을 적용함  
- SGG은 lr에 매우 민감한 편임. 특히 데이터가 sparse이며 features가 서로다른 frequencies (빈도수)를 갖고 있다면, 단 하나의 learning rate가 모든 weight update에 영향을 주는것은 문제가 될 수 있다.

### momentum update
![image](https://user-images.githubusercontent.com/56099627/70980894-73cc4800-20f7-11ea-92c2-1475d65f410c.png)  
- 기존: learning rate 에 바로 x의 위치(gradient)을 업데이트 해줌
- momentum 적용: v(velocity)을 먼저 구한 후, x의 위치(gradient)을 업데이트 해줌
  - mu (뮤): 마찰계수 이며, 일반적으로 0.5~0.99 범위로 사용함(때론, 0.5 에서 0.99 로 mu 값을 증가시켜 사용하기도 함)
  - 경사가 완만할 땐 x의 진행을 천천히, x의 진행을 빠르게 하여 수렴(convergence) 하게 해줌

![image](https://user-images.githubusercontent.com/56099627/70984913-a62d7380-20fe-11ea-8aaf-447acb3a357c.png)
  
(그림) SGD, momentum, nesterov 의 비교

### nesterov Momentum update
![image](https://user-images.githubusercontent.com/56099627/70982968-4ed9d400-20fb-11ea-860a-a6a719521f9d.png)  
  
(그림) momentum 와 nesterov Momentum 비교 : **모멤텀 스텝의 종료점에서 시작** 하여 gradient step을 진행함  
- 파라미터 벡터와 그 위치에서 gradient 을 구하게 되는데 nesterov에선 세타와 다른 위치에서의 gradient을 요구한다. 그러면 일반적인 api 코드와 호환성이 떨어진다. 

![image](https://user-images.githubusercontent.com/56099627/70984362-b2fd9780-20fd-11ea-85db-d5679730cce4.png)  

(그림) nesterov Momentum을 보기 좋은 형태로 ~ 업데이트 할 수 있는 수식 

### AdaGrad update
![image](https://user-images.githubusercontent.com/56099627/70986386-2359e800-2101-11ea-8419-7579f7dae6f7.png)  
- 파라미터 별로 다른 learning rate을 제공해주는 개념 : 파라미터들이 동일한 러닝레이트를 제공받는게 아니라 cache가 계속 building  되어가므로 파라미터 들이 각각 다른 러닝레이트 영향을 받게 된다 (**per- parameter adaptive learning rate method**)
  - (장점) momentum에선 weight 세타에 대해 **동일한 learning rate**을 사용하여 uptate을 하였는데 AdaGra 에선 각가 데이터에 대해 dynamically adapt 함으로서 **각 featrues 마다 서로 다른 lr을 연산**한다.
- 1e-7은 0으로 나누는 일을 방지하기 위한 역할
- (수직) grdient 가 크다는 것은 그만큼 csche 값이 커진다. 그래서 update 되는 곳의 분모에 또한 cache 값이 존재 하므로 분모의 값이 커지고 그만큼 update 진행 느리다. (수평) 경사가 완만하다. grdient 가 작다는 것은 그만큼 csche 값이 작아지고 그래서 update 되는 곳의 분모에 또한 cache 값이 존재 하므로 분모의 값이 작아지므로 그만큼 update 진행 빠르다. 
  - 경사에 경도?? 되지 않는 효과를 가져다 준다?
- 문제점: 스텝 사이즈가 시간에 흐름에 따라서 어떻게 되냐면, cache 값은 시간이 지남에 따라 계속 **building up** 되고 그렇게 진행되다 보면 learning rate 값은 0에 가까운 값이 될것임. 결국 학습이 종료됨
- 학습이 종료되지 않게 해주는 어떤 에너지를 제공해주는데 **RMSprop** 임
- convex problem 에서 시작된 AdaGrad은 convex optimization 에선 잘 작동 하지만 여러개의 레이어로 구성된 NN에서는 다소 적합하지 않음

### RMSprop update
![image](https://user-images.githubusercontent.com/56099627/70991125-37561780-210a-11ea-82ca-496ea4f63a62.png)  
- cache을 서서히 증가시키도록 함으로써 step size 가 0이 되는걸 해결 함
![image](https://user-images.githubusercontent.com/56099627/70991465-d7ac3c00-210a-11ea-8e65-ec0da651f2e4.png)  
- 이건 어떤 논문을 바탕으로 알려진게 아니라, Geoff Hinton's Coursera class 의 lecture 6 에서 "RMSprop 을 쓰면 잘 되더라 ' 라고 알리면서 논문 참조인용 시에 ...(위 그림)... 과 같이 표시하게 됨  

### Adam update
![image](https://user-images.githubusercontent.com/56099627/70992025-1e4e6600-210c-11ea-9ba9-3b8a8efbb20b.png)  
- momentum 과 RMSprop 을 결합한 것으로 adam은 주로 디폴트로 사용됨 
- 초기엔 다소 큰 러닝레이트를 적용하고 빠르게 convergence 하므로 서서히 러닝 레이트를 decay 시키면서 적용하는게 젤 좋은 형태임
  - step decay : 일정한 간격으로 learning rate 시킴
  - exponential decay : <img src="https://latex.codecogs.com/gif.latex?\alpha&space;=\alpha&space;_{0}e^{-kt}" title="\alpha =\alpha _{0}e^{-kt}" /></a> 일반적으로 가장 많이 선택 됨
  - 1/t decay : <img src="https://latex.codecogs.com/gif.latex?\alpha&space;=\frac{\alpha&space;_{0}}{1&plus;kt}" title="\alpha =\frac{\alpha _{0}}{1+kt}" /></a>

# (evaluation) Ensembles 
- 장점) 여러수개의 독립적인 모델을 학습 시킨다. 그런 후, test 시에 이 결과에 대해 평균을 내준다. 이렇게 하면 성능이 약 2% 향상 한다.
- 단점) train : 여러개의 모델을 학습해야 하므로 여러개 모델을 관리 해야 한다, test : 모델이 여러개인 만큼 평균을 내야 하므로 그만큼 테스트 시간이 많아지므로 linear 하게 테스트 속도가 느려된다
- parameter의 앙상블만 해도 성능향상을 할 수 있다.   
- ensemble을 하면 성능이 좋아지는 이유로 '안드레 카파시'가 말하기를~, bowl 형태를 가지는데 이를 최적하는데 미니멈에 가려면 lr이 너무 크면(step size가 크면) 미니멈에 그냥 지나치므로 이 step 들 하나하나를 average을 하면 minimum에 가장 가까운 값을 얻을 수 있을 것임

# (Regularization) Dropout
- 쓰이는데 간단하면서도 성능을 높이는데 많은 영향을 미치므로 매우 중요하다
- batch normalization을 쓰는 경우, dropout을 잘 쓰진 않지만 
![image](https://user-images.githubusercontent.com/56099627/70995050-ed256400-2112-11ea-9500-5e08ee3011c5.png)  
- 일반적인 NN에선 노드 전체가 연결되는 (fully-connected )되는데 dropout은 일부 노드들을 0으로 설정하여(랜덤하게 0으로 설정) 0으로 설정된 노드엔 연결이 되지 않음

![image](https://user-images.githubusercontent.com/56099627/70995330-a421df80-2113-11ea-9354-b3be8e064343.png)  
(그림) 2개층 레이어 에서 dropout 하도록 하는 코드. dropout 비율을 0.5 해둠. 0.5보다 작을 땐 dropout 하도록 코드 작성. 일반적으로 tuple 일 경우 unpack 하기 위해 별표시 함. 

왜 좋은 건지?
- fully connected을 게 되면 중복된 정보가 전달 되는 경향성이 있어
- 하나의 노드가 드롭아웃된 노드의 정보까지 보려하는 ? 노력을 가지어 성능이 향상된다
- 이것도 하나의 앙상블이라고 봄 
- 여러개의 파라미터값들을 평균낸것

- 평가시엔 사용하지 않음
![image](https://user-images.githubusercontent.com/56099627/70996122-7ccc1200-2115-11ea-9243-dc469659d391.png)  
(그림) train 일 때 dropout을 해주고 test 일 때 dropout 계수만큼 곱해주어 스케일링 해줌
  
![image](https://user-images.githubusercontent.com/56099627/70996394-04198580-2116-11ea-8d31-ae2f6ba4bb6e.png)   
(그림) **Inverted Dropout** test 부분은 그대로 두고 그 대신에 train 일 떄 "/p"을 해주어 스케일링을 미리 처리 해줌 <일반적인 방법>

# Convolution neural network
- 1980년 얀 르쿤? 교수가 제안한 모델인 LeNet-5(back-propagation 가능, tanh 사용) 이 상용화된 최초의 CNN 이라 할 수 있는데 
- 2012년 imagenet 대회에서 Alexnet 모델(relu 사용, 네트워크가 훨씬 깊어짐, GPU 발전, weight initialization을 잘 했고 batch normalization을 사용함)로 정확도를 확연히 증가 시킴.  이를 계기로 비약적인 발전

classification 하나의 이미지에 하나의 오브젝트를 찾아내는 것
detection 하나의 이미지에 여러개의 오브젝트를 찾아 내는 것
segmentation 하나의 이미지에 여러개의 오브젝트가 있는데 그 오브젝트들을 따내는 것


참고  
[1] http://cs231n.stanford.edu/2016/syllabus.html, (설명) Andu song  
[2] https://www.youtube.com/watch?v=5t1E3LZ3FDY, cs231n 6강 Training NN part 2  
[3] http://incredible.ai/artificial-intelligence/2017/04/09/Optimizer-Adagrad/, [Optimizer] Adagrad
