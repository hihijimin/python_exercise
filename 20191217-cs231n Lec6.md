Q: 만약에 activation func을 사용하지 않고 그냥 layers을 진행 할 경우? 
단일의 linear 한 func으로 표현 가능해짐
y=ax 라는 linear 함수가 있다면, 1번째 층에선 y=ax이고, 2번째 층에선 y=a(ax), 3번째 층에선 y=a(a(ax)),.. 이런식 일 것임
그래서 은닉계층이 없는 단일 계층을 사용하는 것의 결과가 나타날 것임(즉, linear classification 이 됨)

# Parameter update
### stochastic Gradient 은 왜 느릴까?
![image](https://user-images.githubusercontent.com/56099627/70980180-39ae7680-20f6-11ea-8fd0-a7b11cf981d7.png)  
(그림) loss func 이 수직으로는 경사가 급함, 반면 수평으로는 경사가 완만함. 그래서 수렴하기까지 오랜 시간이 걸려. 이런 문제를 해결하기 위해서 **momentum**을 적용함

![image](https://user-images.githubusercontent.com/56099627/70980894-73cc4800-20f7-11ea-92c2-1475d65f410c.png)  
- 기존: learning rate 에 바로 x의 위치(gradient)을 업데이트 해줌
- momentum 적용: v(velocity)을 먼저 구한 후, x의 위치(gradient)을 업데이트 해줌
  - mu (뮤): 마찰계수 이며, 일반적으로 0.5~0.99 범위로 사용함(때론, 0.5 에서 0.99 로 mu 값을 증가시켜 사용하기도 함)
  - 경사가 완만할 땐 x의 진행을 천천히, x의 진행을 빠르게 하여 수렴(convergence) 하게 해줌

![image](https://user-images.githubusercontent.com/56099627/70981948-7c258280-20f9-11ea-972e-e8b54d2ef6d2.png)  
  
(그림) SGD와 momentum 의 비교




참고  
[1] http://cs231n.stanford.edu/2016/syllabus.html, (설명) Andu song  
[2] https://www.youtube.com/watch?v=5t1E3LZ3FDY, cs231n 6강 Training NN part 2
