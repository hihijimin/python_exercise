# 1. K-nearest neighbor 모델  
가장 간단한 지도학습  
이름을 잘 풀어 생각해 보면 테스트 데이터에서 K 개의 가장 가까운 이웃을 찾아서 그 이웃들 중 다수가 속한 클래스가 그 테스트의 클래스 임  
만약 k=1 이라면, 테스트 데이터 중 가장 가까운 클래스가 그 테스트 데이터의 클래스  
만약 k=3 이라면, 테스트 데이터 중 가장 가까운 3개의 학습데이터를 살펴본 후, 그 중 다수가 속한 클래스가 그 테스트 데이터의 클래스  
  
참고로 K-nearest neighbor은 lazy learning 이라고도 하는데  
테스트 전에 미리 학습 과정을 거치는 선형회귀, 서포트벡터머신 과 같은 다른 지도 학습 알고리즘과 달리 테스트 데이터가 주어졌을 때  
비로소 최근접이웃들을 찾아가는 방식으로 학습을 시작하기 때문에  
  
쉽게 설명하자면 유유상종(매우 직관적인 알고리즘임)  
새로운 데이터가 들어오면 가까이 있는 것이 무엇이냐를 중심으로 그 새로운 데이터 클래스를 정해주는 알고리즘  
근데 무작정 가까이 있는거만 찾으면 큰 오류를 범할 수 있으므로  
k=1,2,3,...등의 조건을 주어 주변에 몇개의 것들을 같이 봐서 많은 것을 골라냄  
k은 입력 데이터로 부터 거리확인용 가상의 원? 이라고 생각하면 되고 가장 가까운 k개의 것들을 확인 해 본다 생각하면 됌  
k가 몇개 해야 좋은지는 모르겠고 데이터 마다 다르게 접근하면 됌  
일반적으로 총데이터의 제곱근값을 사용하고 있음.. 아직도 이부분은 연구중 이램  
  
  
# 2. K-maens clastering  
가장 간단한 비지도 학습 (레이블이 없어요)  
k 개의 클러스터로 분류해주는데 그 순서는  
1) 사용자의 입력받은 k 값에 따라 임의로 클러스터 중심 k 개를 설정해줌  
2) k 개의 클러스터 중심으로 모든 데이터가 얼마나 떨어져 있는지 계산 한 후, 가장 가까운 클러스터 중심을 기준으로 각 데이터들을 클러스터링 해줌  
3) 각 클러스터에 속하는 데이터들의 평균을 계산한 후, 클러스터 중심을 옮겨줌  
4) 보정된 클러스터 중심을 기분으로 2)~3)을 반복함  
5) 더이상 클러스터 중심이 이동하지 않으면 알고리즘 종료  
  
단점: 군집의 크기가 다를 경우, 군집의 밀도가 다를 경우, 데이터 분포가 특이한 경우 제대로 작동하지 않을 수 있음  
  
## 2-1. Hierarchical Clustering 계층적 군집화  
계층적 트리 모형을 이용해 개별 개체들을 순차적으로 계층적으로 유사한 개체 내지 그룹과 통합하여 군집화를 수행하는 알고리즘  
K-maens clastering와 달리 군집 수를 사전에 정하지 않아도 학습을 수행할 수 있음  
  
HC을 수행하려면 모든 개체들 간 거리(distance, 유클리디안 distance 등 ) 나 유사도(similarity)가 이미 계산되어 있어야 함  



  
참고  
[1] https://bskyvision.com/563?category=635506, 유유상종의 진리를 이용한 분류 모델, kNN  
[2] https://gomguard.tistory.com/51, [머신러닝] k-최근접 이웃  
[3] https://bskyvision.com/564?category=635506, 가장 간단한 군집 알고리즘 k-mean 클러스터링  
[4] https://ratsgo.github.io/machine%20learning/2017/04/18/HC/, 계층적 군집화(Hierarchical Clustering)  
