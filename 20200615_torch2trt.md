## 1. torch to tensorRT
https://github.com/NVIDIA-AI-IOT/torch2trt/  

### 환경셋팅
HRnet-pose estimation 환경셋팅 한 conda env에 해줬다

    Option 1 - Without plugins (내장된 model 사용할 때)
    To install without compiling plugins, call the following

    git clone https://github.com/NVIDIA-AI-IOT/torch2trt
    cd torch2trt
    sudo python setup.py install

    Option 2 - With plugins (experimental)(pre_trained model에 적용할 때 )
    To install with plugins to support some operations in PyTorch that are not natviely supported with TensorRT, call the following

    Please note, this currently only includes the interpolate plugin. This plugin requires PyTorch 1.3+ for serialization.

    sudo apt-get install libprotobuf* protobuf-compiler ninja-build
    git clone https://github.com/NVIDIA-AI-IOT/torch2trt
    cd torch2trt
    sudo python setup.py install --plugins
  
### Error 발생
- **ModuleNotFoundError: No module named 'distutils.core'** 해결 위해 distutils 을 설치한다.  
https://github.com/kyuupichan/electrumx/issues/464  
$ sudo apt-get install python3-distutils  
![image](https://user-images.githubusercontent.com/56099627/84639400-49870600-af33-11ea-9e4f-7e00548ee30d.png)  
- **fatal error: Python.h: 그런 파일이나 디렉터리가 없습니다** 해결 위해 python-dev 을 설치한다.  
https://c10106.tistory.com/2600  
$ sudo apt-get install python3-dev  
![image](https://user-images.githubusercontent.com/56099627/84640256-653edc00-af34-11ea-8704-52afeb3585fc.png)  
- **fatal error: NvInfer.h: 그런 파일이나 디렉터리가 없습니다** 해결 위해  
![image](https://user-images.githubusercontent.com/56099627/84867209-c2ae6680-b0b5-11ea-8055-a91e5e5ef21b.png)  
-> tensorTR 설치해야 할듯 NvInfer.h 파일이 tensorRT에 있음...(아래 tensorRT 설치 참고)  
-> 설치한 tensorRT include 파일들을 /usr/local/include/ 또는 /usr/include/ 에 복사하기  
나의 경우엔 /usr/include/ 에 파일(.h)들이 많이 있어 /usr/local/include 에 복사해줬음  
![image](https://user-images.githubusercontent.com/56099627/84857998-3d22ba80-b0a5-11ea-8ea7-57b5410511f9.png)  
$ sudo cp TensorRT-7.0.0.11/include/* /usr/local/include/  
![image](https://user-images.githubusercontent.com/56099627/84858231-bcb08980-b0a5-11ea-8c4a-759c8210f24c.png)  

- **cannot find -lnvinfer**
![image](https://user-images.githubusercontent.com/56099627/84867315-ec678d80-b0b5-11ea-8eeb-9c89a4dadf02.png) 

    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/torch2trt/plugins/interpolate.o -L/usr/lib/aarch64-linux-gnu -L/usr/local/cuda/lib64 -lnvinfer -lcudart -o build/lib.linux-x86_64-3.6/torch2trt/plugins.cpython-36m-x86_64-linux-gnu.so
    /usr/bin/ld: cannot find -lnvinfer
    collect2: error: ld returned 1 exit status
    error: command 'x86_64-linux-gnu-g++' failed with exit status 1

### tensorRT 설치 
0. tensorRT에 대한 전반적인 내용 소개: https://developer.nvidia.com/tensorrt  
tensorRT 깃헙 소개 오픈소스 제공 https://github.com/NVIDIA/TensorRT  
참고! https://eehoeskrap.tistory.com/302  

1. tensorRT 파일 다운로드  
https://developer.nvidia.com/nvidia-tensorrt-7x-download  
![image](https://user-images.githubusercontent.com/56099627/84734958-3f214680-afdd-11ea-81f1-2c044a8be652.png)  

2. tensorRT 압툭 해제  
$ tar xzvf TensorRT-7.0.0.11.Ubuntu-18.04.x86_64-gnu.cuda-10.0.cudnn7.6.tar.gz  

3. LD_LIBRARY_PATH 추가하기  
**실제로 존재하는 TensorRT 경로를 적어야 함**  
export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/media/jimin/D/D_ubuntu/TensorRT-7.0.0.11/lib  
(저장: $ source ~/.bashrc , 확인: $ echo $LD_LIBRARY_PATH)  

4. whl 파일 설치 (- conda 환경 아닌 메인환경에서 설치)  
$ cd TensorRT-7.0.0.11/python  
$ pip3 install tensorrt-7.0.0.11-cp36-none-linux_x86_64.whl (for python 3.x)  
$ cd TensorRT-7.0.0.11/uff  
$ pip3 install uff-0.6.5-py2.py3-none-any.whl  
위치 /usr/local/bin/convert-to-uff 인지 확인 $ which convert-to-uff  
$ cd TensorRT-7.0.0.11/graphsurgeon  
$ pip3 install graphsurgeon-0.4.1-py2.py3-none-any.whl (for python 3.x )  

5. 확인  
**터미널을 한번 껐다가 다시 새로 켜서 확인해야함**  
$ python  
import tensorrt as trt  
![image](https://user-images.githubusercontent.com/56099627/84746937-99c49d80-aff1-11ea-8c57-75fe706e2f0e.png)  

## 2. trt_pose 설치
https://github.com/NVIDIA-AI-IOT/trt_pose  

### 환경셋팅
- 먼저, torch2trt 설치 후, trt_pose 설치한다
```
git clone https://github.com/NVIDIA-AI-IOT/trt_pose
cd trt_pose
python setup.py install
```
![image](https://user-images.githubusercontent.com/56099627/97653415-15f0e680-1aa4-11eb-82ac-028333b55d66.png)  
![image](https://user-images.githubusercontent.com/56099627/97653503-4b95cf80-1aa4-11eb-8d94-320fd9d097b5.png)  

# Doker 이용한 tensorRt 
### 도커 이미지, 컨테이너 셋팅
https://seokhyun2.tistory.com/m/54  

Docker image를 다운받기 위해서 도커 버전이 19.03 이상 이어야함  
Docker image를 다운받아서 활용하시면 되는데, Host의 Nvidia driver 버전에 따라서 활용 할 수 없는 Docker image가 있으므로 링크( https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html )에서 확인 후, 도커 이미지 다운로드 받길  

예를 들면, nvcr.io/nvidia/tensorrt:20.07.1-py3 이미지는 nvidia driver의 version이 450 이상이여야 하는 조건이 있음  
nvidia driver 확인: 440.95 ($ nvidia-smi 으로 확인 가능)  

docker 이미지 및 컨테이너 방법 소개: https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt 
드라이버 440.95, CUDA 버전 10.2 에서 적합한 도커 이미지 다운로드 & 컨테이너 생성(아래)  
```
<이미지 다운로드>
$ docker pull nvcr.io/nvidia/tensorrt:20.03-py3
<공식적인 방법: 컨테이너 생성>
$ docker run --gpus all -it --name trt_test nvcr.io/nvidia/tensorrt:20.03-py3
<내 추천 방법: 컨테이너 생성>
$ docker run --gpus all --shm-size=8g -it --name trt_jm -p 8888:8888 -v "/home/jimin/HDD2/DB":"/workspace/DB" nvcr.io/nvidia/tensorrt:20.03-py3 
$ pip install notebook  
$ jupyter notebook --ip=0.0.0.0 -port=8888 --allow-root  
```
### 도커 컨테이너 안에 torch, torchvision, torch2trt 설치
pip 설치  
$ pip install torch==1.5.0 torchvision==0.6.0  

torch2trt 설치  
$ git clone https://github.com/NVIDIA-AI-IOT/torch2trt.git  
$ cd torch2trt  
$ python setup.py install --plugins  
![image](https://user-images.githubusercontent.com/56099627/108181443-450e8f00-714b-11eb-92b7-0fee59a75961.png)  

### tensorRT 만들기 
https://github.com/NVIDIA-AI-IOT/torch2trt/issues/343  
```
def convert_trt(path):
    model = eval('models.' + cfg.MODEL.NAME + '.get_pose_net')(
        cfg, is_train=False
    ).cuda()
    model.load_state_dict(torch.load(path, map_location=device))
    model.eval()

    print("Start converting")
    x = torch.ones((1, 3, cfg.DATASET.INPUT_SIZE, cfg.DATASET.INPUT_SIZE)).cuda()
    model_trt = torch2trt.torch2trt(model, [x], fp16_mode=True, max_workspace_size=1 << 26)

    print("Start saving")
    torch.save(model_trt.state_dict(), os.path.join(os.path.dirname(path), cfg.MODEL.NAME + '_trt.pth'))
    # torch.save(model_trt.state_dict(), 'HigherHRNet_trt.pth')
    print("Finish")
```
```
import torch
from torch2trt import torch2trt
from torch2trt import TRTModule
from models.hrnet import HRNet
import os

def convert_trt(path):
#     model = eval('models.' + cfg.MODEL.NAME + '.get_pose_net')(
#         cfg, is_train=False
#     ).cuda()
#     model.load_state_dict(torch.load(path, map_location=device))
#     model.eval()

    print("Start converting")
    x = torch.ones((1, 3, 256, 192)).cuda()
    model_trt = torch2trt(model, [x])

    print("Start saving")
    torch.save(model_trt.state_dict(), os.path.join(os.path.dirname(path), os.path.basename(path).split('.pth')[0] + '_trt.pth'))
    # torch.save(model_trt.state_dict(), 'HigherHRNet_trt.pth')
    print("Finish convert")

def LoadExecute_trt(model, path):
    ## tensorRT load
    trt_path = os.path.join(os.path.dirname(path), os.path.basename(path).split('.pth')[0] + '_trt.pth')
    
    model_trt = TRTModule()
    model_trt.load_state_dict(torch.load(trt_path))
    print('Load trt')
                        
    ## tensorRT execute
    # 임의 데이터 만들기
    test_data = torch.randn(1, 3, 256, 192).cuda()
                        
    y = model(test_data)
    print('--model ok')
    y_trt = model_trt(test_data)
    print('--trt_model ok')
    # check the output against PyTorch
    print(torch.max(torch.abs(y - y_trt)))
    
#hrnet_c 32
checkpoint_path= '/workspace/torch2trt/models/pose_hrnet_w32_256x192.pth'
device = 'cuda:0'

model = HRNet(c=32, nof_joints=17)
checkpoint = torch.load(checkpoint_path, map_location=device)

if 'model' in checkpoint:
    model.load_state_dict(checkpoint['model'])
else:
    model.load_state_dict(checkpoint)
    
model = model.to(device)

model.eval()

convert_trt(checkpoint_path)

LoadExecute_trt(model, checkpoint_path)
    
```

