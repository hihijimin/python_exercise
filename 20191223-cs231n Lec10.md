# Recurrent neural networks
![image](https://user-images.githubusercontent.com/56099627/71337998-88925b00-2591-11ea-9e87-60e772815648.png)  
- vanilla neural networks
- image Captioning : image -> sequence of words
- sentiment classification: 감정 분류의 예 sqeuence of words -> sentiment
- machine translation : seq of words -> seq of words
- video classification on frame level

### fixed inputs 을 sequential processing 한 경우
<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/71338919-fd1ac900-2594-11ea-8cb9-9aa43d0a3e91.png" /></p>  
- conv net에 큰 이미지를 feed 해서 번지수를 classification 하는 방법 대신에 작은 conv을 이용해서 이미지 전체를 sequential 하게 훑어 나가게 된다. 왼쪽에서 오른쪽으로 가면서 숫자를 읽어 나감
### fixed outputs 을 sequential processing 한 경우

### RNN
![image](https://user-images.githubusercontent.com/56099627/71339634-6b608b00-2597-11ea-97d6-35ea75eb2b3b.png)  
- 시간의 흐름에 따라 input벡터를 받게 된다 즉 매 타임스텝마다 input벡더가 rnn으로 입력되게 되는데 rnn은 내부적으로 어떤상태를 가지게 된다 그리고 그 상태를 func(매 타임스텝마디 인풋으로 받는 func)으로 변형해 줄수 있다. rnn도 weight으로 구성이 되며 weight 들이 튜닝됨에 따라서 rnn이 진화되어 나가기 때문에 새로운 iput이 들어올때마다 다른 반응을 보인다. 특정 타임스탭에서의 예측을 원하게 된다(우리가 원하는 결과행동을 갖게함)  
- 주의점: **every time step 마다 동일한 func(,parameter set 등)을 사용한다**  
- 이럴경우, input sequence와 output sequence 사이즈가 매우 크더라도 무관하게 rnn을 적용할 수 있다.

### (vanilla) Recurrent neural network
- state가 single hidden vector **h**으로만 구성됨












참고  
[1] http://cs231n.stanford.edu/2016/syllabus.html, (설명) Andu song  
[2] https://www.youtube.com/watch?v=2ngo9-YCxzY&t=1623s, cs231n 10강 RNN, LSTM
