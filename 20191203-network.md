# 1. 딥하게 네트워크 구조를 해야 하는 이유?
네트워크 계층 구조를 기억해 두고, 만약에 개를 인식하는 문제를 생각해보면,  
layer가 한개라면 한번에 '이헤' 해야 하기 때문에  
신경망을 깊게 한다면, 학습해야 할 문제를 계층적으로 분해할 수 있어.  
( CNN 계층에서는 에지 등의 단순한 패턴에 뉴런이 반응하고 층이 깊어지면서 질감과 같이 점차 복잡한 것에 반응한다. )  
또 층을 깊게하면 정보를 계층적으로 전달할 수 있어  
예, 에지를 추출한 다음 층은 에지 정보를 쓸수 있고 더 고도의 패턴을 효과적으로 학습하리라 기대할 수 있어  
즉, 층을 깊게 함으로써 각 층이 학습해야 할 문제를 '풀기 위한 단순한 문제'로 분해 할수 있어 효율적임  
  
# 2. VGG 
합성곱 계층과 풀링 계층으로 구성되는 기본적인 CNN  
네크워크 깊이를 깊게 만드는게 성능에 어떤 영향을 미치는지를 확인 하고자 함(vgg 연구원팀)  
3X3 의 작은 필터를 사용한 합성곱 계층을 연속적으로 거친다.  
(필터 사이즈가 크면 그만큼 이미지 사이즈가 금방 축소되기 때문에 네트워크 깊이를 충분히 깊게 만들기 불가능하다)  
'3*3 필터로 두차례 콘볼루션 하는 것 = 5x5 필터 한번 콘볼루션' 인거 알지?  
**3x3 필터 2번 콘볼루션 하는 장점은?**  
3x3 필터가 2개 이면 총 18게 가중치를 갖고, 5x5 필터 1개는 25개 가중치를 갖기 때문에 3*3 필터 2개가 가중치가 작아 학습 속도가 빨라지며 층의 갯수가 늘어나면서 특성에 비선형성을 더 증가 시키기 때문에 특성이 더 유용해짐  
  
인풋 224x224x3  
1층 conv1_1 : 64개 3x3x3 필터커널로 입력 이미지를 컨볼루션, ( zeropadding=1, stride=1 : 모든 층 동일)  
1층 conv1_1 결과 : 64장의 224x224 특성맵 (224x224x64)  
1층 conv1_1 활성화 : ReLu 함수 적용 (모든 층에서 적용, 마지막 층은 적용 안됌)  
2층 conv1_2 : 64개 3x3x64 필터커널로 특성맵을 컨볼루션  
2층 conv1_2 결과 : 64장의 224x224 특성맵 (224x224x64)  
2층 conv1_2 풀링 : 2x2 최대풀링을 stride=2 적용하면 112x112x64 특성맵  
  
3층 conv2_1 : 128개 3x3x64 필터커널로 특성맵을 컨볼루션  
3층 conv2_1 결과 : 128장의 112x112 특성맵 (112x112x128)  
4층 conv2_2 : 128개 3x3x128 필터커널로 특성맵을 컨볼루션  
4층 conv2_2 결과 : 128장의 112x112 특성맵 (112x112x128)  
4층 conv2_2 풀링 : 2x2 최대풀링을 stride=2 적용하면 56x56x128 특성맵  
  
5층 conv3_1 : 256개 3x3x128 필터커널로 특성맵을 컨볼루션  
5층 conv3_1 결과 : 256장의 56x56 특성맵 (56x56x256)  
6층 conv3_2 : 256개의 3x3x256 필터커널로 특성맵 컨볼루션  
6층 conv3_2 결과 : 256장의 56x56 특성맵 (56x56x256)  
7층 conv3_3 : 256개의 3x3x256 필터커널로 특성맵을 컨볼루션  
7층 conv3_3 결과 : 256장의 56x56 특성맵 (56x56x256)  
7층 conv3_3 풀링 : 2x2 최대풀링을 stride=2 적용하면 28x28x256 특성맵  
  
8층 conv4_1 : 512개의 3x3x256 필터커널 특성맵을 컨볼루션  
8층 conv4_1 결과 : 512개의 28x28x256 특성맵 (28x28x512)  
9층 conv4_2 : 512개의 3x3x512 필터커널 특성맵을 컨볼루션  
9층 conv4_2 결과 : 512개의 28x28x512 특성맵 (28x28x512)  
10층 conv4_3 : 512개의 3x3x256 필터커널 특성맵을 컨볼루션  
10층 conv4_3 풀링 : 2x2 최대풀링을 stride=2 적용하면 14x14x512 특성맵  
10층 conv4_4 결과 : 512개의 14x14x512 특성맵 (14x14x512)  
  
11층 conv5_1 : 512개의 3x3x256 필터커널 특성맵을 컨볼루션  
11층 conv5_1 결과 : 512개의 14x14x512 특성맵 (14x14x512)  
12층 conv5_2 : 512개의 3x3x512 필터커널 특성맵을 컨볼루션  
12층 conv5_2 결과 : 512개의 14x14x512 특성맵 (14x14x512)  
  
13층 conv5_3 : 512개의 3x3x512 필터커널 특성맵을 컨볼루션  
13층 conv5_3 풀링 : 2x2 최대풀링 stride=2 적용하면 7x7x512  
13층 conv5_3 결과 : 512개의 7x7x512 특성맵 (7x7x512)  
  
14층 fully connect 1 : 4096개 7x7x512 필터커널 특성맵 컨볼루션  
14층 fully connect 1 결과 : 4096개의 1x1 특성맵 (4096개의 뉴런)  
15층 fully connect 2 : 4096개의 뉴런, fc1층의 4096개의 뉴런과 연결. 훈련시 dropout 적용  
16층 fully connect 3 : 10000개의 뉴런, fc2층의 4096개의 뉴런과 연결  
16층 fully connect 3 출력 : softmax 함수로 활성화, 1000개 클래스로 분류된 네트워크  
  
# 3. LeNet
CNN 중에 조상격?, 최종버전 LeNet-5, 유명한 CNN 구조중 가장 간단한 구조  
LeNet은 CNN을 처음으로 개발한 얀르쿤(Yann Lecun)연구팀이 1998년에 개발함  
  
C : Convolution layer, S : subsampling layer  
convolution 에서 훈련해야할 파라미터 개수 : (가중치x 입력맵개수 + 바이어스)x특성맵개수   
subsampling 에서 훈련해야할 파라미터 개수 : (입력맵개수 + 바이어스)x특성맵개수  
fully connect 에서 훈련해야할 파라미터 개수 : (입력개수 + 바이어스)x출력개수  
  
**subsampling 에서 평균 풀링인데 왜 훈련해야할 파라미터가 필요한가?**  
평균을 낸 후, 한개의 훈련 가능한 가중치를 곱해주고 또 한개의 훈련가능한 바이어스를 더해준다. 그 값들이 시그모이드 함수를 통해 활성화 된다. 그 가중치와 바이어스는 시그모이드의 비활성도를 조절해준다.  
  
인풋 32x3  
C1 : 인풋 - 6개의 5x5 필터 간의 컨볼루션 연산  
C1 결과 : 6장의 28x28 특성맵, 훈련해야할 파라미터 개수= (5x5x1 +1)x6 =156  
  
S2 : 6장의 28x28 특성맵 에 대해 서브 샘플링  
S2 결과 : 14x14 특성맵, 줄어든 이유: 2x2 필터를 stride=2로 설정해서 서브샘플링 해주기 때문, 서브샘플링 방법은 average pooling, 훈련해야할 파라미터 개수: (1+1)*6=12  
  
C3 : 6장의 14x14 특성맵에 컨볼루션 연산  
C3 결과 : 16장의 10x10  ?? , 456+606+303+151 =총 1516  
C3 결과 추가설명 :  
6장의 14x14 특성맵에서 연속된 3장씩 모아 5x5x3 필터와 컨볼루션 = 6장의 10x10 특성맵, (5x5x3+1)x6 =456  
6장의 14x14 특성맵에서 연속됭 4장씩 모아 5x5x4 필터와 컨볼루션 = 6장의 10x10 특성맵, (5x5x4+1)x6 =606  
6장의 14x14 특성맵에서 불연속된 4장씩 모아 5x5x4 필터와 컨볼루션 = 3장의 10x10 특성맵, (5x5x4+1)x3 =303  
6장의 14x14 특성맵에서 모두를 가지고 5x5x6 필터와 컨볼루션 = 1장의 10x10 특성맵, (5x5x6+1)x1 =151  
  
S4 : 16장의 10x10 특성맵 에 대해 서브 샘플링  
S4 결과 : 16장의 5x5 특성맵, (1+1)x16 =32  
  
C5 : 16장의 5x5 특성맵 - 120개 5x5x16 필터와 컨볼루션  
C5 결과 : 120x1x1 특성맵, (5x5x16 +1)x120 =48120  
  
FC6 : 84개의 유닛을 가진 feedforward network. 120*1*1 특성맵 - 84 유닛 간의 연결, 훈련해야할 파라미터 개수: (120+1)x84 =10164  
  
아웃풋 10개의 euclidean radial basis function 유닛들로 구성됨  
  
LeNet-5 를 제대로 가동하기 위해 훈련해야 할 파라미터 156+12+32+48120+10164 = 총60000  
  
  
# 4. AlexNet
CNN 부흥에 아주 큰 역할 한 구조  
2개의 GPU로 병렬연산을 수행하기 위해 병렬적인 구조로 설계됨  
  
zero-padding 이란?  
컨볼루션으로 인해 특성맵 사이즈가 축소되는 것을 방지 하기 위해 맵 가장자리를 0으로 추가하는 것  
  
인풋 224x224x3  
  
layer 1 : 입력에 96개의 11x11x3 필터커널 사용해 컨볼루션, stride=4, zero-padding 없음  
layer 1 결과 : 96장의 55x55 특성맵 (55x55x96)  
layer 1 활성화 : ReLu 함수 활성화  
layer 1 풀링 : 3x3 overlapping max pooling 에서 stride =2 적용  
layer 1 풀링 결과 : 96장의 27x27 특성맵 (27x27x96)  
layer 1 local response normalization: 수렴속도를 높이기 위해 실행, 특성맵 차원 변화시키지 않음(27x27x96 유지)  
  
layer 2 : 전단계 특성맵에 256개의 5x5x48 필터커널 사용해 컨볼루션, stride=1, zero-padding =2  
layer 2 결과 : 256개의 27x27 특성맵 (27x27x256)  
layer 2 활성화 : ReLu 함수 활성화  
layer 2 풀링 : 3x3 overlapping max pooling 에서 stride =2 적용  
layer 2 풀링 결과 : 256장의 13x13 특성맵 (13x13x256)  
layer 2 local response normalization : 수렴속도를 높이기 위해 실행, 특성맵 차원 변화시키지 않음(13X13X256 유지)  
  
layer 3 : 전단계 특성맵에 384개의 3x3x256 필터커널 사용해 컨볼루션, stride=1, zero-padding =1  
layer 3 결과 : 384장의 13x13 특성맵 (13x13x384)  
layer 3 활성화 : ReLu 함수 활성화  
  
layer 4 : 전단계 특성맵에 384개의 3x3x192 필터커널 사용해 컨볼루션, stride=1, zero-padding =1  
layer 4 결과 : 384개의 13x13 특성맵 (13x13x384)  
layer 4 활성화 : ReLu 함수 활성화  
  
layer 5 : 전단계 특성맵에 256개의 3x3x192 필터커널 사용해 컨볼루션, stride=1, zero-padding =1  
layer 5 결과 ; 256개의 13x13 특성맵  
layer 5 활성화 : ReLu 함수 활성화  
layer 5 풀링 : 3x3 overlapping max pooling 에서 stride =2 적용  
layer 5 풀링 결과 : 256장의 6x6 특성맵 (6x6x256)  

fully connected layer  
layer 6 : 전단계 특성맵에 4096개의 6x6x256 필터커널 사용해 컨볼루션  
layer 6 결과 : 4096개의 1x1 특성맵 (1x1x4096)  
layer 6 활성화 및 출력 : ReLu 함수 활성화, 4096개의 성분(뉴런)으로 구성된 벡터 생성  
  
layer 7 : 전단계 뉴런과 연결 (4096개의 뉴런으로 구성)  
layer 7 활성화 및 출력 : ReLu 함수 활성화  
  
layer 8 : 전단계 뉴런과 연결 (1000개 뉴런으로 구성)  
layer 8 활성화 및 출력 : softmax 함수 적용해 1000개 클래스에 속할 확률 추출  

*LeNet-5에서 6만개 파라미터가 훈련되야 했던것에 반해 AlexNet에선 6천만개 파라미터가 훈련되어야 함.
그만큼 컴퓨터 기술도 좋아졌고 훈련시간을 줄이기 위한 방법들도 사용되었기에 가능함*
  
**Relu ?**  
retified linear unit의 약자  
LeNet-5에선 tanh 사용했는데 alexnet에선 Relu 사용함  
tanh보단 6배가 빠르다고 함 그래서 주로 렐루 선호  
  
**Dropout ?**  
over-ffting을 막기 위한 규제 기술  
fully-connected layer의 뉴런 중 일부를 생략하면서 학습을 진행하는 것  
몇몇 뉴런의 값을 0으로 바꿔버리므로 그 뉴런들은 forward pass 와 back propagation 에 아무런 영향을 미치지 않음  
dropout은 훈련시에만 적용되는 것으로 평가 시엔 모든 뉴런을 사용한다  
  
**overlapping pooling ?**  
CNN에서 pooling의 역할은 컨볼루션을 통해 얻은 특성맵의 크기를 줄이기 위함인데  
LesNet에선 평균풀링 average pooling 사용된 반면, Alexnet에선 maxpooling 사용됨  
LesNet에선 stride 사이즈를 커널보다 작게 하는 overlapping pooling 사용함  
즉, LesNet에선 non-overlapping average pooling 이고 Alexnet에선 overlapping maxpooling 사용함  
overlapping 풀링이 에러율을 줄이는데 도움  
  
**local respose normalization (LRN) ?**  
말 그대로 데이터 값을 정규화 시킴  
주변 이웃 데이터에 대해 정규화를 실행하는 것  
  
**data augmentation ?**  
over-ffting을 막기 위한 방법으로 데이터 양을 늘리기  
데이터 양이 적으면 과적합 될 확률이 높으므로  
alexnet 개발자들은 data augmentation 방법으로 데이터 양 늘려서 했음( 하나의 이미지로 여러장의 비슷한 이미지를 만듦)  
하나의 이미지로 여러장 만드는 방법은 좌우 반전, 허용하는 입력 이미지 크기를 조금 다르게 함, 같은 내용을 담지만 위치를 조금 다르게 함  
  
# 5. googleNet
### <특징1> 1x1 컨볼루션
1x1 필터커널로 컨볼루션 해주는 연산이 많다...  
1x1 컨볼루션은 특성맵의 갯수를 줄이는 목적으로 사용된다 -> 특성맵 갯수가 적으면 그만큼 연산량이 줄어든다.  
연산량을 줄일 수 있다는 점은 네트워크를 더 깊이 만들수 있게 도와준다 준다  
**특성맵에 따른 연산량 계산을 해보면?**  
14x14x480 (480장의 14x14 특성맵) 을 48개의 5*5*480 필터커널로 컨볼루션 하면, (padding =2, stride =1)
14x14x48 (48장의 14x14 특성맵) 생성  
연산횟수 = (14x14x48)x(5x5x480) =약 112.9M  
  
14x14x480 (480장의 14x14 특성맵) 을 16개의 5x5x16 필터커널로 컨볼루션 하면, (padding =2, stride =1)  
14x14x16 (16장의 14x14 특성맵) 생성  
**how to transfer from 480 to 16 ??**  
그니까 2번 과정을 거치는데 14x14x480 을 16개의 1x1x480 필터커널로 컨볼루션 한 후(14x14x16), 48개의 5x5x16 필터커널로 컨볼루션 하면, 14x14x48 (48장의 14x14 특성맵) 생성  
연산횟수 = (14x14x16)x(1x1x480) + (14x14x48)x(5x5x16) =약 5.3M  
  
### <특징2> Inception 모듈
9개의 inception 모듈을 포함하고 있음  
filter concatenation  
1x1 convolution, 3x3 convolution, 5x5 convolution, 1x1 convolution  
                 1x1 convolution, 1x1 convolution, 3x3 max pooling  
previous layer  
이전층에서 생성된 특성맵을 1x1 convolution, 3x3 convolution, 5x5 convolution, 3x3 max pooling 모두 함께 쌓아준다.  
좀 더 다양한 종류의 특성이 도출된다. (동일한 사이즈의 필터커널을 이용해 컨볼루션을 해준거에 비해)  
  
### <특징3> Global average pooling
Alexnet , Vggnet 등에선 fully connectec layer 가 망의 후반부에 연결되어 있으나 googlenet은 FC 방식 대신에 global average pooling 방식을 사용한다.  
이것은 이전층에서 산출된 특성맵들을 각각 평균낸 것들을 쭉~ 이어서 1차원 벡터를 만들어 준다.  
이후 softmax층을 연결해 줄수 있기 때문에 1차원 벡터로 해줌  
FC 방식은 1024개의 7x7x1024 필터커널로 컨볼루션 한다면 필요한 가중치는 7x7x1024x1024 =51.3M 인 반면, GAP 방식은 1024개의 7x7x1 필터커널로 사용하면 필요한 가중치는 없다.  
  
## <특징4> autiliary classifier
네트워크 깊이가 길어지면 깊어질수록 vanising gradient문제가 발생하는데  
가중치를 훈련하는 과정에서 역전파를 주로 활용하는데 역전파과정에서 가중치를 업데이트 하는데 사용되는 gradient가 점점 작아져서 0이 되어버리는 현상..  
그 결과 네트워크 내의 가중치들이 제대로 훈련되지 않는다.  
이 문제를 극복하기 위해서 Googlenet에선 네트퉈크 중에서 보조 분류기(autilliary classifier)을 달아주었음   
보조 분류기 구성은  
5x5 평균 풀링(stride=3) - 128개의 1x1 필터커널 콘볼루션 - 1024 fully connected - 1000 fully connected - softmax  
보조 분류기 구성은 훈련시에만 사용하고 평가시엔 사용하지 않는다  



  
  
  
<참고자료>  
[1] 밑바닥 부터 시작하는 딥러닝 8장 p267  
[2] https://bskyvision.com/504, VGGNet의 구조(VGG16)  
[3] https://bskyvision.com/418, Lenet-5의 구조  
[4] https://bskyvision.com/421?category=635506, Alexnet 구조  
[5] https://bskyvision.com/539?category=635506, googlenet 구조  
