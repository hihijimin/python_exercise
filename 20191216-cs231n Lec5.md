![image](https://user-images.githubusercontent.com/56099627/70966237-edebd500-20d5-11ea-83e9-9f984b8aee8d.png)
(그림설명) 1번: 이미지넷 데이터로 학습된 모델을 가져와서(가중치들을 가져옴) - 2번: 내가 가진 데이터가 매우 작을 때 : 레이어의 마지막 부분에만 내가 가진 데이터로 training을 시킨다 (마지막 softmax 부분을 교체 시킨다) - 3번: 또는 내가 가진 데이터가 매우 작은 편은 아니지만 학습에 적용하기엔 작을 땐: 네트워크의 아랫쪽 부분을 추가로 학습하는 fine-tunning을 시킴
  
# Bottleneck feature
<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/70964972-5c7a6400-20d1-11ea-92af-14abc5471e3d.png" /></p>  
- 어파인 레이어(Fully-connected layer) 바로전 CNN 블록을 **Bottleneck feature** 이라 함  
- CNN 모델은 각 CNN 블록의 풀링(pooling) 레이어를 지나면서 feature size가 줄어들기(= 추상화되기) 때문에 feature size를 기준으로 생각하면 병을 뒤집어둔 모양과 비슷하다.  
  
# Fine tuning 이란?
- **기존에 학습된 모델을 가져와서(weight 포함) 내가 가진 데이터를 추가하여 학습시켜 파라미터를 업데이트 하는 방법**  
  - 주의점은 정교해야 한다. 전체 파라미터가 망가지는 문제가 발생하지  
- 기존에 학습된 모델을 가져와서 내가 가진 데이터를 마지막 부분Fully-connected layer(Affine layer 또는 Dense layer))에만 내가 가진 데이터로 training을 시킨다 (마지막 softmax 부분을 교체 시킨다) **이건 fine tuning 아님**  
  - fine tuning이 아닌 이유? 피쳐를 추출해내는 레이어의 파라미터를 업데이트 하지 않았기 때문에, affine layer 사용했으니 fine tuning이라 생각할 수 있지만, 기존의 레이어를 사용한게 아니라 새로 구성한 affine layer 이므로  
   
# Neural Network history
### Perceptron (~1957)
- perceptron 은 NN의 시조
- activation func 이 binery step func 이라 미분 불가 (-> back propagation이 불가)
- 가중치를 '**임의대로**' 조절하면서 네트워크를 최적화 시키려 했던거임
- circuit 기반 하드웨어에서 사용

### Adaline/Madeline (~1960)
<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/70900100-75d0d100-203b-11ea-837f-04817e47a26b.png" /></p>  
- perceptron을 쌓아가기(stacking) 시작함
- 최초의 multi layer perceptron Network
- 여전히 Backpropagation 불가 
- **parametric approach(weight) 도입** 하였지만 여전히 결과는 좋지 못했음
- 하드웨어에서 사용
- 이후 1970~1980년대 중반 까지는 암흑기

### 최초 back-propagation 도입(1986년)
- back-propagation 은 미분이 가능하다는 것이고 가중치를 체계적으로 찾아나갈수 있다는 것임
- 생각보다 잘 동작하지 않음, 특히 네트워크가 더욱 deep 해지면 제대로 동작하지 않았음
- 이후 2000년대 중반 까지 제 2의 암흑기

### back-propagation 제대로 동작 가능(2006년)
- 20년 만에 back-propagation이 제대로 동작이 가능해짐
- deep learning 이름이 시작됨
-RBM(Restricted Boltzman machine)을 이용해 별도의 unsupervised 된 pre training을 한다. 

### 딥러닝 폭팔적 발전 (2010, 2012년)
- weight initialation 할 수 있는 방법을 찾음
- sigmoid 외에 제대로 된 activation func을 적용할 수 있게됨
- GPU을 원할하게 사용할 수 있는 환경 조성
- data가 폭팔적으로 늘어난 점

# 신경망에 관련된 설정들
### activation func
### sigmoid func
- sigmoid func 은 전통적으로 가장 많이 사용 되었지만 더이상은 잘 사용되지 않은 함수
- 넓은 범위의 숫자를 0~1 사이를 스퀘시 해서 해줌 들어오는 입력 값에 대해 가중치의 영향력을 주기에 굉장히 적합했기에 많이 사용했음
- 하지만 더이상 사용되지 못할 문제(3가지)
  - 뉴런이 포화(saturated neurons)되서 gradient을 없애버리는 가장 큰 문제점 **vanishing gradient**
  <p align="center"><img width="40%" src="https://user-images.githubusercontent.com/56099627/70902479-3bb5fe00-2040-11ea-82f2-6639b83b3853.png" /></p> 
    - local gradient x global gradeient = gradient 인데 이때 local gradient을 자세히 보면, x의 값이 매우 작거나(x=-10) x의 값이 매우 크거나(x=10) 할때 미분은 0에 가까운 값이 되므로(local gradient = 0) vanishing gradient 가 된다
    - 결과적으로 back-propagation 멈추게 된다.
  - sigmoid의 output은 zero-centered 가 아님
  - exp()은 성능에 저하를 가져다 줌

### tanh(x)
<p align="center"><img width="40%" src="https://user-images.githubusercontent.com/56099627/70903162-9e5bc980-2041-11ea-9ee7-97d380cd7b28.png" /></p>  
- range (-1 ~ 1), zero centered (아주 좋은 특성)
-  x의 값이 매우 작거나(x=-10) x의 값이 매우 크거나(x=10) 할때 미분은 0에 가까운 값이 되므로(local gradient = 0) **vanishing gradient**

### ReLu (Rectified Linear Unit)
<p align="center"><img width="40%" src="https://user-images.githubusercontent.com/56099627/70903691-c0098080-2042-11ea-9456-f219c37590fd.png" /></p>  
- computes f(x) =max(0,x)
- x가 양수인 지점에서는 saturation이 발생하지 않을 것이고 (기울기가 1이 되므로)
- 빠른 converges (sigmoid/tanh에 비해)
- 2012년 alexnet 연구진이 제안한 내용임
- (단점) zero-centered output이 아님
- (단점) x <0 일 때 기울기가 0이 되어서 **vanishing gradient** 생김

<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/70904244-39ee3980-2044-11ea-816e-a2fc258f3bd6.png" /></p>  
- 데이터들이 데이터클라우드 내에 activation된 경우, active ReLu 
- 데이터들이 데이터클라우드 외부에서 activation된 경우, dead ReLu (절대로 activation 되지 않고 그래서 update 되지 않음)
  - dead ReLu 되는 경우는 운이 나쁘면 **초기화 할때** dead ReLu Zone에서 시작하는 경우가 있지
    - 그래서, 초기화 할때, biase 값을 0 이 아닌 0.01 으로 함 (논란의 여지 있음)
  - 학습 할때, learning rate가 너무 크게 할 때 dead ReLu가 발생한다

### Leaky ReLu
<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/70904517-efb98800-2044-11ea-88d0-81a4adf39048.png" /></p>  
- f(x) = max(0.01x, x)
- x< 0 또는 x>0 상관없이 saturate 되지 않을 것임 -> gradient kill 발생하지 않을 것임
- ReLu 보다 분명 upgrade 된 함수이지만 아직.. 검증 단계 이라 분명히 좋은 함수라고 말하기엔.. 

### ELU (Exponential Linear Units)
<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/70904705-5dfe4a80-2045-11ea-9b0c-39f16728312a.png" /></p> 
<img src="https://latex.codecogs.com/gif.latex?f(x)=\left\{\begin{matrix}&space;x&&space;(if&space;&x>0)\\&space;\alpha(e^{x}-1)&space;&&space;(if&space;&x\leq&space;0)&space;\end{matrix}\right." title="f(x)=\left\{\begin{matrix} x& (if &x>0)\\ \alpha(e^{x}-1) & (if &x\leq 0) \end{matrix}\right." /></a>
- Relu의 모든 장점을 가지면서 vanishing gradient 가 되지 않으며 zero mean output에 가까운 형태를 가짐
- (단점) exp() 연산은 연산량이 큰 것이라 연산 할때 다소 무리? 함

### activation func 정리
- 기본적으론 Relu 사용 할것이며
- Relu으로 잘 되지 않을 경우, leaky Relu/ Maxout/ ELU 사용을 추천
- tanh은 가급적 사용하지 않는게 좋고 sigmoid는 더이상 사용하지 않는 것이 좋음(하지만 여전히 LST<에선 쓰임)

# DATA Pre pocessing
![image](https://user-images.githubusercontent.com/56099627/70906059-d87c9980-2048-11ea-8887-45ea00d45915.png)  
![image](https://user-images.githubusercontent.com/56099627/70906031-c569c980-2048-11ea-8c39-78b0648c2a79.png)  
- 일반적으로 zero-centered data 과정을 기본적으로 해주지만 normalization은 일반적으로 수행하지 않는다. 왜냐하면 특정범위에 들어가도록 하는데 이미지라는 것은 기본적으로 0~255 범위를 가지는 픽셀이므로  이미 특정 범위에 있으므로 굳이 해줄 필요가 없으므로  
- PCA 데이터를 비상관화 하여 차원을 줄이는 방법으로 Whitening은  이미지간에 인접한 ? 중복한 값을 줄어들수 있도록 해주는 작업임 하지만 일반적으로 이미지 전처리 과정으로 PCA와 whiten 과정을 해주진 않는다  
- 정리하자면, 이미지에선 전처리로 zero-centered 만 신경을 써주면 된다
  - CIFAR-10  데이터의 경우(32x32x3), 이미지 평균 값을 빼준다(AlexNet 에서 방법) 과 channel 별로 평균값을 빼준다(vgg 에서의 방법) 
    - channel 별로 평균값을 빼준다(vgg 에서의 방법)이 훨씬 편리할 것임

# Weight initialization
### 만약 모든 가중치가 0으로 초기화 된다면, 
- 모든 뉴런들이 동일한 연산을 수행
- back-propagation은 동일한 gradient 연산을 수행 할 것임
- 이렇게 동일한 연산을 수행하는 것은 symetric breaking이 발생하지 않는다는 의미
- 가중치를 초기화 하는 아이디어는 랜덤 숫자를 사용하되 매우작은 랜덤 넘버를 사용한다. 평균이 0이고 표준편차가 1e-2인 값을 가지는 gaussian 형태를 사용한다.
이렇게 구성한다면 네트워크가 작을 경우는 상관없지만, 네트워크가 커지게 된다면 문제가 생김

- 모든 activation은 0이 되어버림
- 반면, 0.01대신에 1.0을 넣으면 오버슈팅이 되어 모든 뉴런들이 saturation 된다. (결과적으로 -1, 1 에 포화됨) 결과적으로 gradient도 0이 되어버림 

### Xavier initialzaiton (2010년 발표)
**W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)**
- 인풋을 개수가 많으면 그 만큼 나눠주기 때문에 weight 값이 적어질 것임 (초기화 방법)
- 인풋의 개수가 적으면 그 만큼 나눠주기 때문에 weight 값이 커질 것임 (초기화 방법)
- tanh 에선 잘 적용이 되는 하지만 ReLu에선 잘 적용이 안됨
  - 문제점 해결을 위해 **W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in /2)** 사용하였더니 activation func이 relu인 경우 잘 작동을 함 , 즉 relu에 최적화된 방법임

### Batch normalization (2015년 발표)
초기값 설정에 너무 연연해 하지 않고 할 수 있는 방법이 있는데 그 방법이 Batch normalization  
이전과정에서 activation func으로 relu을 쓰고 weight initialization 를 하는 방법등을 하여 vanoishing gradient 발생을 줄이는 즉 간접적인 방법으로 했었는데 간접적인 방법 말고 학습하는 과정을 전반적으로 안정화를 위해서 학습속도를 가속화 시키면서도 안정적인 학습 할 수 있도록 근본적으로 방법을 제시한 방법  
불안정화는 내부에서 covriance shift가 일어난다고 봄 레이어를 거치면서 입력값의 분포가 달라지는 현상이 있어서 불안정화 일어난다 봄
각 레이어를 거칠 때마다 normalization을 해주자 라는 컨셉
방법: 정규화 시키기 -> 감마 noramalize에 스케일링 하는 것, 베타 shift 시키는 거 (감마, 베타는 학습을 부여, 정규화 조정 단계)
  
학습할때 mean, variance을 미리 계산을 해두고 나중에 테스를 할 때는 전체의 값에 mean, variance으로 batch noramlization을 적용시켜 준다.  
  
# Learning Process 관리
proprecessing - network architecture 구성 - loss 체크 - training data의 일부분만 가지고 확인하여 overfitting 일어나는지 확인 - learning rate 


# Hyperparameter Optimization
- random search : 
- grid search : 일정한 간격으로 
- learning rate 또는 regularation을 구할 때는 ramdom search을 사용하자 (grid search 말고)
 
- 하이퍼 파라메터를 찾는 방법에는 여러가지가 존재하는데
- network architecture 에서 node는 몇개 할 것인지, 또는 hidden layer는 몇개 할 것인지, 어떤 모델을 사용 할 것인가 등등
- learning rate 과 그 learning rate의 decay schedule을 어떻게 잡아줄 것인지, update type은 무엇으로 할지
- regularization은 L2? Dropout 할것인지 등등
  
- bad initialization 경우 : 초기엔 로스가 정체되다가 시간이 어느정도 시간이 지나면 학습이 진행되는(로스가 줄어드는) 경우
- weight updates / weight magnitude (want 약 ~1e-3)
  
  
참고  
[1] https://www.youtube.com/watch?v=gPIxeIwUkAc, cs231n 5강 Training NN part 1  
[2] https://eehoeskrap.tistory.com/186, [Deep Learning] pre-training 과 fine-tuning (파인튜닝)
[3] http://blog.haandol.com/2016/12/25/define-bottleneck-feature-and-fine-tuning.html, Bottleneck feature? Fine-tuning?
