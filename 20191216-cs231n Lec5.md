# NN history
### perceptron (~1957)
- perceptron 은 NN의 시조
- activation func 이 binery step func 이라 미분 불가 (-> back propagation이 불가)
- 가중치를 '**임의대로**' 조절하면서 네트워크를 최적화 시키려 했던거임
- 하드웨어에서 사용

### Adaline/Madeline (~1960)
<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/70900100-75d0d100-203b-11ea-837f-04817e47a26b.png" /></p>  
- perceptron을 쌓아가기(stacking) 시작함
- 최초의 multi layer perceptron Network
- 여전히 Backpropagation 불가 
- **parametric approach(weight) 도입** 하였지만 여전히 결과는 좋지 못했음
- 하드웨어에서 사용
- 이후 1970~1980년대 중반 까지는 암흑기

### 최초 back-propagation 도입(1986년)
- back-propagation 은 미분이 가능하다는 것이고 가중치를 체계적으로 찾아나갈수 있다는 것임
- 생각보다 잘 동작하지 않음, 특히 네트워크가 더욱 deep 해지면 제대로 동작하지 않았음
- 이후 2000년대 중반 까지 제 2의 암흑기

### back-propagation 제대로 동작 가능(2006년)
- 20년 만에 back-propagation이 제대로 동작이 가능해짐
- deep learning 이름이 시작됨
-RBM(Restricted Boltzman machine)을 이용해 별도의 unsupervised 된 pre training을 한다. 

### 딥러닝 폭팔적 발전 (2010, 2012년)
- weight initialation 할 수 있는 방법을 찾음
- sigmoid 외에 제대로 된 activation func을 적용할 수 있게됨
- GPU을 원할하게 사용할 수 있는 환경 조성
- data가 폭팔적으로 늘어난 점

# 신경망에 관련된 설정들
### activation func
### sigmoid func
- sigmoid func 은 전통적으로 가장 많이 사용 되었지만 더이상은 잘 사용되지 않은 함수
- 넓은 범위의 숫자를 0~1 사이를 스퀘시 해서 해줌 들어오는 입력 값에 대해 가중치의 영향력을 주기에 굉장히 적합했기에 많이 사용했음
- 하지만 더이상 사용되지 못할 문제(3가지)
  - 뉴런이 포화(saturated neurons)되서 gradient을 없애버리는 가장 큰 문제점 **vanishing gradient**
  <p align="center"><img width="40%" src="https://user-images.githubusercontent.com/56099627/70902479-3bb5fe00-2040-11ea-82f2-6639b83b3853.png" /></p> 
    - local gradient x global gradeient = gradient 인데 이때 local gradient을 자세히 보면, x의 값이 매우 작거나(x=-10) x의 값이 매우 크거나(x=10) 할때 미분은 0에 가까운 값이 되므로(local gradient = 0) vanishing gradient 가 된다
    - 결과적으로 back-propagation 멈추게 된다.
  - sigmoid의 output은 zero-centered 가 아님
  - exp()은 성능에 저하를 가져다 줌

### tanh(x)
<p align="center"><img width="40%" src="https://user-images.githubusercontent.com/56099627/70903162-9e5bc980-2041-11ea-9ee7-97d380cd7b28.png" /></p>  
- range (-1 ~ 1), zero centered (아주 좋은 특성)
-  x의 값이 매우 작거나(x=-10) x의 값이 매우 크거나(x=10) 할때 미분은 0에 가까운 값이 되므로(local gradient = 0) **vanishing gradient**

### ReLu (Rectified Linear Unit)
<p align="center"><img width="40%" src="https://user-images.githubusercontent.com/56099627/70903691-c0098080-2042-11ea-9456-f219c37590fd.png" /></p>  
- computes f(x) =max(0,x)
- x가 양수인 지점에서는 saturation이 발생하지 않을 것이고 (기울기가 1이 되므로)
- 빠른 converges (sigmoid/tanh에 비해)
- 2012년 alexnet 연구진이 제안한 내용임
- (단점) zero-centered output이 아님
- (단점) x <0 일 때 기울기가 0이 되어서 **vanishing gradient** 생김

<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/70904244-39ee3980-2044-11ea-816e-a2fc258f3bd6.png" /></p>  
- 데이터들이 데이터클라우드 내에 activation된 경우, active ReLu 
- 데이터들이 데이터클라우드 외부에서 activation된 경우, dead ReLu (절대로 activation 되지 않고 그래서 update 되지 않음)
  - dead ReLu 되는 경우는 운이 나쁘면 **초기화 할때** dead ReLu Zone에서 시작하는 경우가 있지
    - 그래서, 초기화 할때, biase 값을 0 이 아닌 0.01 으로 함 (논란의 여지 있음)
  - 학습 할때, learning rate가 너무 크게 할 때 dead ReLu가 발생한다

### Leaky ReLu
<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/70904517-efb98800-2044-11ea-88d0-81a4adf39048.png" /></p>  
- f(x) = max(0.01x, x)
- x< 0 또는 x>0 상관없이 saturate 되지 않을 것임 -> gradient kill 발생하지 않을 것임
- ReLu 보다 분명 upgrade 된 함수이지만 아직.. 검증 단계 이라 분명히 좋은 함수라고 말하기엔.. 

### ELU (Exponential Linear Units)
<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/70904705-5dfe4a80-2045-11ea-9b0c-39f16728312a.png" /></p> 
<img src="https://latex.codecogs.com/gif.latex?f(x)=\left\{\begin{matrix}&space;x&&space;(if&space;&x>0)\\&space;\alpha(e^{x}-1)&space;&&space;(if&space;&x\leq&space;0)&space;\end{matrix}\right." title="f(x)=\left\{\begin{matrix} x& (if &x>0)\\ \alpha(e^{x}-1) & (if &x\leq 0) \end{matrix}\right." /></a>
- Relu의 모든 장점을 가지면서 vanishing gradient 가 되지 않으며 zero mean output에 가까운 형태를 가짐
- (단점) exp() 연산은 연산량이 큰 것이라 연산 할때 다소 무리? 함

### activation func 정리
- 기본적으론 Relu 사용 할것이며
- Relu으로 잘 되지 않을 경우, leaky Relu/ Maxout/ ELU 사용을 추천
- tanh은 가급적 사용하지 않는게 좋고 sigmoid는 더이상 사용하지 않는 것이 좋음(하지만 여전히 LST<에선 쓰임)

# DATA Pre pocessing
![image](https://user-images.githubusercontent.com/56099627/70906059-d87c9980-2048-11ea-8887-45ea00d45915.png)
![image](https://user-images.githubusercontent.com/56099627/70906031-c569c980-2048-11ea-8c39-78b0648c2a79.png)
- 일반적으로 zero-centered data 과정을 기본적으로 해주지만 normalization은 일반적으로 수행하지 않는다. 왜냐하면 특정범위에 들어가도록 하는데 이미지라는 것은 기본적으로 0~255 범위를 가지는 픽셀이므로  이미 특정 범위에 있으므로 굳이 해줄 필요가 없으므로
- PCA 데이터를 비상관화 하여 차원을 줄이는 방법으로 Whitening은  이미지간에 인접한 ? 중복한 값을 줄어들수 있도록 해주는 작업임 하지만 일반적으로 이미지 전처리 과정으로 PCA와 whiten 과정을 해주진 않는다
- 정리하자면, 이미지에선 전처리로 zero-centered 만 신경을 써주면 된다
  - CIFAR-10  데이터의 경우(32x32x3), 이미지 평균 값을 빼준다(AlexNet 에서 방법) 과 channel 별로 평균값을 빼준다(vgg 에서의 방법) 
    - channel 별로 평균값을 빼준다(vgg 에서의 방법)이 훨씬 편리할 것임



참고  
[1] 
