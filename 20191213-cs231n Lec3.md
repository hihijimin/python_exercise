Loss function Optimization

## multiclass SVM loss : hinge loss
![image](https://user-images.githubusercontent.com/56099627/70791748-078dd380-1ddb-11ea-91ac-d07b1625ffa7.png)  
![image](https://user-images.githubusercontent.com/56099627/70791913-723f0f00-1ddb-11ea-92a7-7b7bb4958f53.png)  
  
**hinge loss (SVM loss)**
max(0, sj - sy +1) : 둘중 더 큰 값을 취하겠다
sj : 잘못된 레이블의 score, sy : 제대로된 레이블의 score, 1 : safety margin
최종적으로 loss = (2.9 + 0 + 12.9) / 3 = 4.6
Q1 참고로 알아둬~ : +1 만큼 해주기 때문에 +1 해준만큼 최종 loss 값이 높아질수 밖에 없다
Q2 각각의 loss 값을 구한 뒤 sum 해주고 나서 평균값 구하는데, 아예 각각 loss 구할때 평균 값 해준 뒤 각각을 더해주면 어떻겠느냐? 음... 결과적으로 비슷하지 않을까? 전체 합 하고 평균하거나 각 평균 후 합치거나~
Q3 max(0, sj - sy +1)^2 해준뒤 sum 하는건 어때? 제곱하는거 자체가 linear 하지 않게 됨(non-linear)
Q4 max 값에서 나올수 있는 범위(min/max)? max(0, sj - sy +1)의 좌/우 값 중에서 min은 0이 되겠고, max은 무한대가 되겠다
Q5 초기값을 weight를 0에 가까운 수로 설정? 하는데 loss 는 얼마가 되겠는가?
![image](https://user-images.githubusercontent.com/56099627/70793369-7e789b80-1dde-11ea-8391-6c95cb0992fe.png)  

loss = (data loss)학습용 데이터를 최적화 하려 노력함 + (regularization)lambda : 일반화 하려고 노력함
사실 regularization하는 과정을 넣으면 학습하는 과정 및 결과는 안좋아지지만 테스트 셋에 대한 퍼포먼스는 더 좋아진다.
regularization 입장에선 weight가 0이 되길 좋아하지 왜냐면 loss가 작아지므로 
하지만 다른편으로 생각하면 regularization 입장에선 절대 weight가 0이 될수 없어 왜냐면 분류를 해야하므로 
결론적으로 data loss 와 regularization 간의 싸움? 을 통해 더 훌륭한 결과를 낼수 있는 거

![image](https://user-images.githubusercontent.com/56099627/70795267-c1d50900-1de2-11ea-9f6a-c99d2ec4825f.png)  
regularization 입장에선 w1 w2 둘중 어떤 걸 선호 할까?? **W2**
이유 : w1은 1번 벡터에만 있어 1번 에만 영향력 있구, w2은 전체 벡터에 값이 존재해서 전체에 영향을 줄수 있어 
모든 인풋 피쳐들을 고려하길 원한다(diffuse over everything)
즉 동일한 아웃풋이라면 spread out 되는 것을 선호합니다

## softmax classifier : softmax loss
![image](https://user-images.githubusercontent.com/56099627/70796052-aa971b00-1de4-11ea-8cb8-4bcb384d1275.png)  
softmax classifier = multinomial logistic regression : 이항에 대한 로지스틱 리그레션을 다차원으로 일반화 한것이다.
**score 정의 : 클래스를 log화 한 확률. nomalize 하지 않은 ~**
**Loss = 정확한 class의 (-)마이너스 로그의 확률을 최소화 하자**
**이것을 Cross Entropy Loss 라고 부름**

Q5 초기값을 weight를 0에 가까운 수로 설정? 하는데 soft max classifier 의 loss 는 얼마가 되겠는가?
