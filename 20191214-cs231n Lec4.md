# Backpropagation and NN part 1   
forward 시엔 local gradient을 구할 수 있고
backward 시엔 global gradient을 구할 수 있음
그러므로 결국엔 local gradient와 global gradient을 구해서 gradient을 구할 수 있다.
backward 시에 chain rule 이 일어나기에 가능한거임

![image](https://user-images.githubusercontent.com/56099627/70844554-6197a300-1e86-11ea-8a0d-c355f5709502.png)
  
  
(그림설명) 더하기(+)는 gradient distributer 즉, 그냥 전파해주는 느낌, 곱하기(x)은 gradient switcher 즉, 곱해주는 대상자를 서로 바꿔줌


![image](https://user-images.githubusercontent.com/56099627/70844620-41b4af00-1e87-11ea-82a2-85db57b34aa7.png)  
(그림설명) sigmoid는 미분 하면 (1-자기자신)(자기자신) 꼴로 나오게 된다. 그러면 이 과정을 (0.73)(1-0.73)=0.2으로 계산 가능하다. 





참고  
[1] https://www.youtube.com/watch?v=qtINaHvngm8&list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5&index=3, cs231n 4강 Backpropagation and NN part 1  
[2] https://cs231n.stanford.edu/2016/syllabus.html, (설명)Andy Song 
