# Backpropagation and NN part 1   
forward 시엔 local gradient을 구할 수 있고
backward 시엔 global gradient을 구할 수 있음
그러므로 결국엔 local gradient와 global gradient을 구해서 gradient을 구할 수 있다.
backward 시에 chain rule 이 일어나기에 가능한거임

![image](https://user-images.githubusercontent.com/56099627/70844554-6197a300-1e86-11ea-8a0d-c355f5709502.png)
  
  
(그림설명) 더하기(+)는 gradient distributer 즉, 그냥 전파해주는 느낌, 곱하기(x)은 gradient switcher 즉, 곱해주는 대상자를 서로 바꿔줌, MAX gate은 'router 즉, 둘 중 값이 큰 쪽만 취해준다.큰 쪽에 1이 전파되고 작은 쪽은 0을 전파해줄거임 


![image](https://user-images.githubusercontent.com/56099627/70844620-41b4af00-1e87-11ea-82a2-85db57b34aa7.png)  
(그림설명) sigmoid는 미분 하면 (1-자기자신)(자기자신) 꼴로 나오게 된다. 그러면 이 과정을 (0.73)(1-0.73)=0.2으로 계산 가능하다. 

# Neural Network
- linear score func : f = Wx
- 2-layer Neural Network : f = W_2 max( 0, W_1x)
  - max 함수는 activation 함수 중 하나인 Lelu임
  - 기본적으로 activation 함수는 non-linearity을 제공한다

![image](https://user-images.githubusercontent.com/56099627/70844941-12547100-1e8c-11ea-90c2-c1db16a7cd19.png)  
  

참고  
[1] https://www.youtube.com/watch?v=qtINaHvngm8&list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5&index=3, cs231n 4강 Backpropagation and NN part 1  
[2] https://cs231n.stanford.edu/2016/syllabus.html, (설명)Andy Song 
