# Backpropagation and NN part 1   
![image](https://user-images.githubusercontent.com/56099627/70888301-5c238f80-2023-11ea-91b4-92266db86240.png)  
(그림설명) computation~ 은 간략하게 나타내어 늘어놓고 계산 할 수 있지만 이걸 복잡한 구조일 경우, 늘어놓고 계산 한는건 힘들 것임  
  
<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/70889040-0223c980-2025-11ea-92ef-2d048ddad8ff.png" /></p>  
AlexNet  
input image은 weight 과정을 거쳐서 마지막에 loss을 구할 수 있게 됨  
  
<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/70889112-267fa600-2025-11ea-997c-1984963fa495.png" /></p>  
Neural Turing Machine  
input 에서 loss을 구할 때까지 어마어마한 규모로 구성되어 있음을 확인 되지요  
  
<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/70889247-79f1f400-2025-11ea-8811-f9bdd0adc555.png" /></p>  
만약에 recureent neural network의 경우, 이런 어마어마한 규모의 구성이 수백장이 복사가 되는 식이라 너무 너무 거대한 규모라 한꺼번에 모듈을 계산하는 일은 말이 되지 않는다.  
  
**그래서 (간단한 예를 통해) 하나하나 모듈별로 계산해 나가는 방법을 학습해 보자**  
  
![image](https://user-images.githubusercontent.com/56099627/70890331-22a15300-2028-11ea-9d2a-7208a2be5042.png)  
forward 시엔 local gradient을 구할 수 있고  
backward 시엔 global gradient을 구할 수 있음  
그러므로 결국엔 local gradient와 global gradient을 구해서 gradient을 구할 수 있다.  
backward 시에 chain rule 이 일어나기에 가능한거임  
  
  
![image](https://user-images.githubusercontent.com/56099627/70844554-6197a300-1e86-11ea-8a0d-c355f5709502.png)
  
(그림설명) 더하기(+)는 gradient distributer 즉, 그냥 전파해주는 느낌, 곱하기(x)은 gradient switcher 즉, 곱해주는 대상자를 서로 바꿔줌, MAX gate은 'router 즉, 둘 중 값이 큰 쪽만 취해준다.큰 쪽에 1이 전파되고 작은 쪽은 0을 전파해줄거임 


![image](https://user-images.githubusercontent.com/56099627/70844620-41b4af00-1e87-11ea-82a2-85db57b34aa7.png)  
(그림설명) sigmoid는 미분 하면 (1-자기자신)(자기자신) 꼴로 나오게 된다. 그러면 이 과정을 (0.73)(1-0.73)=0.2으로 계산 가능하다.  
  
# Neural Network
- linear score func : f = Wx  
- 2-layer Neural Network : f = W_2 max( 0, W_1x)  
  - max 함수는 activation 함수 중 하나인 Lelu임  
  - 기본적으로 activation 함수는 non-linearity을 제공한다  
- 3-layer Neural Network : W_3 max( 0, W_2 max( 0, W_1x) )  
  
Data driven approach 에서 non-parametric approach 와 parametric approach 가 있는데  
- non-parametric approach의 대표적인 예는 nearest neighbor 인데  
  - 이것은 1개의 class에 1개의 classifier 만 존재한다  
- parametric approach 의 대표적인 예는 Neural network, CNN 같은 경우  
  - 이것은 1개의 class에 대해 여러개의 classifier가 존재한다  
  
  
![image](https://user-images.githubusercontent.com/56099627/70844941-12547100-1e8c-11ea-90c2-c1db16a7cd19.png)  
(그림) sigmoid은 x가 아무리 작아도 y는 0이 되고 x가 아무리 커도 y는 1이 되는  
  
<p align="center"><img width="40%" src="https://user-images.githubusercontent.com/56099627/70895242-ea9f0d80-2031-11ea-9e0b-7dc260b4a006.png" /></p>  
(그림설명) 모든 레이어 들이 연결되어 있다고 해서 'fully connected layers' 라고 함  
  

<p align="center"><img width="40%" src="https://user-images.githubusercontent.com/56099627/70895837-fdfea880-2032-11ea-9901-6bed15d7bec3.png" /></p>  
NN에서 hidden neurons 가 많아 질수록 classification 능력이 커진는데  
  
<p align="center"><img width="40%" src="https://user-images.githubusercontent.com/56099627/70895872-0eaf1e80-2033-11ea-8e6e-96a5b7e1580d.png" /></p>  
하지만 일반화(generalation) 시키는 것은 Regularization strength와 연관성 있다. lambda= 0.001 보다 lambda= 0.1 일때 일반화가 된다
  
**Neural network에서 overfitting 일어나지 않기 위핸 방법으로 네트워크를 작게 만드는 것(hidden layer 개수 작게)이 아니라, regularization strength을 높여준다.**    
  
  
참고  
[1] https://www.youtube.com/watch?v=qtINaHvngm8&list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5&index=3, cs231n 4강 Backpropagation and NN part 1  
[2] https://cs231n.stanford.edu/2016/syllabus.html, (설명)Andy Song 
