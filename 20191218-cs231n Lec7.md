# Convolution Neural Network
![image](https://user-images.githubusercontent.com/56099627/71087314-3d69f800-21df-11ea-8322-398623787d31.png)  
(그림) 32x32 image 위에 5x5 filter을 convolution 해주면 28x28x1 activation map 이 생성  
- 이미지 위에 필터를 필터 시킨다. 1 filter 1 actiavation map  
- ( ( image의 width또는 hight - filter의 width또는 hight + 2(padding size) ) / stride ) +1 = (32-5/1) +1
  
![image](https://user-images.githubusercontent.com/56099627/71087342-4ce94100-21df-11ea-8583-0114b736efd9.png)    
(그림) 28x28x6 인 새로운 이미지로 **represetation** 되었다고 보면 됨. 이것이 다음의 conv layer의 인풋으로 들어감.  
  
![image](https://user-images.githubusercontent.com/56099627/71087620-fcbeae80-21df-11ea-8366-fc1265b31dcd.png)  
(그림) 이것이 Convolution Neural Network 과정의 기본적인 형태임. 궁극적으로 업데이트 해나가야 할 것은 **필터의 값들**임.
  
![image](https://user-images.githubusercontent.com/56099627/71056490-c7dd3800-219c-11ea-96aa-20dd5bb87ecf.png)  
(그림) 바로 앞단에 기반하여 visualization 한 것임 : 1st filter은 이미지를 visualization 하게 된 것이고 2nd filter을 visualization 한것임. 더 깊은 feature 일수록 더 추상화된 filter을 볼수 있음.(2013년에 확인된 사실?)  
  
![image](https://user-images.githubusercontent.com/56099627/71056960-6cac4500-219e-11ea-9bd7-edc506a03346.png)  
(그림) **32개의 5x5 filters 가 있는데 각각이 주는 filter map 이 생성됨** (activation 에서 하얀색 부분이 activation이 높은지점) filter와 image 간의 convolution 한 것임.
  
![image](https://user-images.githubusercontent.com/56099627/71057133-0247d480-219f-11ea-9cf3-eda060aab8d2.png)  
(그림) 일반적으로 이런식으로 구성됨 : (conv-relu-conv-relu-pooling) - ... - Fullyconnected convolution(이 단계에서 score가 진행되고 classification 됨) , conv layer에선 계속해서 사이즈를 유지해지므로 pooling layer가 필요함, 10개의 filter에서 10개의 activation map을 생성함. 이 10개의 activation map 전체를 volume 이라고 말함  
  
- **padding을 쓰면 이미지 사이즈를 보존할 수 있다**
  - 왜 중요할까? padding을 이용해서 size을 보존해 줌. padding을 안하면 voulme 자체가 계속 줄어들게 된다(0이 되어버리면 더이상 conv 진행을 못하게 됨) 즉, convenience, re-presentation 차원에서 유용하다. 때론 size을 줄이는 것(downsampling)이 의미가 있을 때도 있음(-> 이건 pooling layer에서 관련 있음)

![image](https://user-images.githubusercontent.com/56099627/71063483-1a741f80-21b0-11ea-82e5-ca6d14affaa1.png)
(그림) 생성되는 파라미터 개수(=필터의 개수)는?
  - 일반적으로 filter의 개수를 일반적으로 2의 n승 으로 해줌. 왜냐하면 convenience, perfomance 측면에서 유리 하므로
  
![image](https://user-images.githubusercontent.com/56099627/71088932-07c70e00-21e3-11ea-9028-d0c554b7a9df.png)  
(그림) filter 개수가 5라 하면 depth가 5. conv layer은 28x28x5 임. 각각의 activation map에서 각각의 뉴런들은 인풋이미지의 동일한 위치를 쳐다본다 반면에 각각 다른 activation map을 가지는 (뉴런들)얘들은 서로 다른 weight(parameters)을 공유하지 않는다.
- 동일한 depth(같은 activation map) 내에 있는 뉴런들은 서로 뉴런간에 parameters을 공유 하지만
- 별개의 activation map엔 속하지만 동일한 위치에 있는 뉴런들은 인풋이미지에 같은곳을 쳐다본다

# Pooling layer
volume 을 작게 representation 하고 좀 더 manageable 할 수 있게 해줌, 각각의 activation map을 독립적으로 작용할 수 있도록 해줌
<p align="center"><img width="20%" src="https://user-images.githubusercontent.com/56099627/71064887-77bda000-21b3-11ea-9bed-0d59ab141041.png" /></p> 
(그림) volume 은 224x224x1 이고 224x224x64 가 activation map 임

# Fully connected layer

안드레 카파시? 교수가 자바로 개발한 ... 사이트
http:/cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html

### case: LeNet-5
conv, pooling, fully connected 

6 28x28 feature map 
6 14x14 feature map

### case: AlexNet (2012년)
![image](https://user-images.githubusercontent.com/56099627/71069931-13074300-21bd-11ea-9fad-6a8a0a07b335.png)  
- 2개의 GPU을 가지고 CNN을 했기에 인풋이후 부터 2개로 나눠 CNN 되고 있음
- alrexnet에선 normalization을 사용했었는데 지금은 효용이 없다해서 안쓰고 있음
- 최초로 Relu을 사용했음, data augmnetation을 사용, dropout 0.5, batch size 128, lr 1e-2, L2
input : 227x227x3  
1st conv1 : 55x55x96 : 96 11x11 filters, 4 stride, 55= (227-11)/4+1 : parameters 34,848 = 11x11x3x96  
2nd pool1 : 27x27x96 : 3x3 filters, 2 stride, 27= (55-3)/2+1 : parameters = 0 (파라미터는 conv에만 있음)  
(중략)

# case: ZFNet (2013년)
![image](https://user-images.githubusercontent.com/56099627/71076522-359f5900-21c9-11ea-9566-a3c30c43c93a.png)  
- 기본적으로 Alexnet과 유사함 
  - conv1에서 11x11 filters(stride 4)은 크기 때문에 7x7 filters(stride 2)으로 바꿨고
  - conv3,4,5에서 384,384,256 filter 개수를 512, 1024, 512 더 늘려줌

# case: VggNet (2014년 2등)
- 7.3% 에러를 줄임
- conv나 maxpooling layer에서 filter 사이즈를 계속 변경하며 했었는데 vgg에선 오직 **3x3 conv(stride 1 pad 1), 2x2 maxpooling(stride 2)** 으로 함
<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/71077389-aa26c780-21ca-11ea-9b0b-7d73ba22f0ce.png" /></p>  
- 몇개의 weight layer을 했을때 최적의 결과가 나오는지 탐구한? 결과 임
![image](https://user-images.githubusercontent.com/56099627/71082320-3be70280-21d4-11ea-8bf6-12aee0411f7a.png)  
- 메모리 사용량은 conv에서 앞쪽에 3.2M 대부분 소모, 뒤쪽에 FC 을 보면 1.3억개 이상의 파라미터를 사용하는데 이건 효율적이지 않다는 의견? 때문에 최근에는 average pooling을 사용하는 것으로 (avaage pooling 방법 예: 512개의 7x7 을 average pooling 하여 단일 columns으로 변환 해줌), FC 만큼 잘 동작하면서 parameter 개수를 대폭 줄어주므로 매우 효율적이다

# case: GoogleNet (2014년 1등)
![image](https://user-images.githubusercontent.com/56099627/71083481-75b90880-21d6-11ea-8f4e-476b087e2888.png)  
<p align="center"><img width="80%" src="https://user-images.githubusercontent.com/56099627/71083395-44d8d380-21d6-11ea-8800-82703ac4c56b.png" /></p>
- inception module이 연속으로 연결된 형태임, 6.3% 에러를 줄임
- 파라미터를 vgg에선 1.3억개인데 googlenet에선 5만개로 줄임, 또 alexent 파라미터에 비해 1/12 수준이지만연산은 2배이상 빠르며 에러도 6.67%으로 좋은(16.4% 비해) 

# case: ResNet (2015년)
![image](https://user-images.githubusercontent.com/56099627/71083709-fc6de580-21d6-11ea-8800-442e931bf65f.png)  
(그림) imagenet 데이터 뿐만아니라 CoCo 데이터에서도 좋은 성능을 보임, classification, Detection, localization, segmentation에서도 효과적으로 성능을 보이게 됨  
![image](https://user-images.githubusercontent.com/56099627/71083609-c7619300-21d6-11ea-85e8-8a564fca33be.png)  
(그림) 2015년에 layer 수가 폭팔적으로 증가하였고, 에러도 3.57으로 마니 줄어들었음
![image](https://user-images.githubusercontent.com/56099627/71084027-b49b8e00-21d7-11ea-8a90-93952f172cca.png)  
(그림) CIFAR-10 실험을 했는데, Resnet을 제외한 vgg,alrexnet 등등에선 layers을 더 추가할 수록 오히려 에러가 증가함을 보임, 하지만 resnet은 layers을 추가할 수록 에러가 감소함을 보인다. 그래서 resnet을 제외한 모델들은 최적화에 실패한 것들이라고 봄. 결로적으로 점점더 많은 layers을 추가하기 위해선 resnet 방법을 따라야 한다고 주장함.

- alexnet은 8 layers, vgg은 19 layers, resnet은 152 layers 이라서 resnet을 학습시키는데 8GPU 환경에서 약 2~3주 걸리지만 평가시에는 vgg 보단 빠른 결과가 나온다
- 초반에 224x224x3 이미지를 7x7 conv (64개, stride=2) 으로 해줌으로써 56x56 활성맵 으로 사이즈를 굉장히 줄여줌으로서 이후 작업이 효율적으로 연산이 가능해짐 skip connection이 들어가서 효율적으로 연산가능해짐
  
![image](https://user-images.githubusercontent.com/56099627/71085075-004f3700-21da-11ea-8acf-003a16141033.png)  
<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/71085437-e3ffca00-21da-11ea-92db-83905c89f066.png" /></p> 
(그림) skip connection이 있고 마지막에 더하기(+)연산이 있는데 더하기 연산은 distribute 역할을 하기에 때문에 빠르게 연산이 가능하다. 즉 back-propagation 할 때 순식간에 앞쪽 연산으로 갈수 있는 터전?을 마련해 둔 것이다.
- 모든 conv layer 이후엔 batch normalization을 사용함, SGD + momentum(0.9) 사용, lr =0.1이며 에러가 정체되면 그때마다 lr을 10으로 나눴음, batch normlization을 사용했기때문에 dropout을 사용하지 않았음
![image](https://user-images.githubusercontent.com/56099627/71085899-e282d180-21db-11ea-812a-a154ccfb3e98.png)  
(그림) 1x1 conv을 사용했음      이부분은 나중에 추가로 찾아서 적어둘 것임...

![image](https://user-images.githubusercontent.com/56099627/71086054-37264c80-21dc-11ea-8356-2d207f118edf.png)  

# 정리
- convnet은 conv, pooling, Fullyconnected layers을 사용하고 있으며
- 최근엔 주로 3x3 filter size와 같은 작은 필터 사이즈를 사용하고 더 깊은 architectures 사용한다
- 또한 pooling/ fulling connectec layers을 주로 사용하지 않고 conv layers 만을 사용하는 추세이다
  - 전형적인 architectures : ( (conv - relu)x N_pooling )x M - (fully conneted - relu)xK , (마지막단) softmax




  
  
참고  
[1] http://cs231n.stanford.edu/2016/syllabus.html, (설명) Andu song  
[2] https://www.youtube.com/watch?v=rdTCxAM1I0I, cs231n 7강 CNN  
