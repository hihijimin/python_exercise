1. 환경
```
torch               1.7.0
torch2trt           0.1.0
torchvision         0.8.0
tensorrt            7.2.1.6
onnx                1.6.0
onnxruntime         1.6.0
```
## torch To onnx
2. 코드
```
import json
#import trt_pose.coco
#import trt_pose.models
import torch
import torch2trt
from torch2trt import TRTModule
import time, sys
import cv2
import torchvision.transforms as transforms
import PIL.Image
#from trt_pose.draw_objects import DrawObjects
#from trt_pose.parse_objects import ParseObjects
import argparse
import os.path

import numpy as np
import os
import sys
import matplotlib.pyplot as plt
#sys.path.append('/opt/nvidia/deepstream/deepstream-5.0/sources/ai_plant/pose/torch2trt')
from models.hrnet import HRNet
print(os.getcwd())
#sys.path.append('/opt/nvidia/deepstream/deepstream-5.0/sources/ai_plant/pose')

def joints_dict():
    joints = {
        "coco": {
            "keypoints": {
                0: "nose",
                1: "left_eye",
                2: "right_eye",
                3: "left_ear",
                4: "right_ear",
                5: "left_shoulder",
                6: "right_shoulder",
                7: "left_elbow",
                8: "right_elbow",
                9: "left_wrist",
                10: "right_wrist",
                11: "left_hip",
                12: "right_hip",
                13: "left_knee",
                14: "right_knee",
                15: "left_ankle",
                16: "right_ankle"
            },
            "skeleton": [
                [15, 13], [13, 11], [16, 14], [14, 12], [11, 12], [5, 11], [6, 12], [5, 6], [5, 7],
                [6, 8], [7, 9], [8, 10], [1, 2], [0, 1], [0, 2], [1, 3], [2, 4],  # [3, 5], [4, 6]
                [0, 5], [0, 6]
            ]
        },
    }
    return joints


def preprocess(image):
    ## Create Image transform
    """
    transform = transforms.Compose([
                    transforms.ToPILImage(), # 이미지 데이터를 PIL image로 바꿔줌
                    transforms.Resize((HEIGHT, WIDTH)), #(256,192)
                    transforms.ToTensor(), # 이미지 데이터를 tensor로 바꿔줌
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # 이미지를 정규화
                ])
    """
    global device
    device = torch.device('cuda')
    
    mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()
    std = torch.Tensor([0.229, 0.224, 0.225]).cuda()
    
    image = cv2.resize(src, dsize=(WIDTH, HEIGHT), interpolation=cv2.INTER_AREA)
    
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = PIL.Image.fromarray(image) # transform 하기 위해선 PIL 형태로 바꿔야 함
    image = transforms.functional.to_tensor(image).to(device)
    image.sub_(mean[:, None, None]).div_(std[:, None, None])
    
    return image[None, ...]

'''
Draw to inference (small)image
'''
def execute(img):
    start = time.time()
    data = preprocess(img)
    cmap, paf = model_trt(data)
    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()
    end = time.time()
    counts, objects, peaks = parse_objects(cmap, paf)#, cmap_threshold=0.15, link_threshold=0.15)
    for i in range(counts[0]):
        print("Human index:%d "%( i ))
        get_keypoint(objects, i, peaks)
    print("Human count:%d len:%d "%(counts[0], len(counts)))
    print('===== Net FPS :%f ====='%( 1 / (end - start)))
    draw_objects(img, counts, objects, peaks)
    return img

'''
Draw to original image
'''
def execute_2(org): 
    print(org.shape)
    data = preprocess(org)
    print(data.shape)
    out = model_trt(data)
    out = out.detach().cpu().numpy() # out = out.detach().cpu()
    print(type(out), out.shape)
    pts = np.empty((out.shape[0], out.shape[1], 3), dtype=np.float32)
    # For each human, for each joint: y, x, confidence
    for i, human in enumerate(out):
        for j, joint in enumerate(human):
            pt = np.unravel_index(np.argmax(joint), (HEIGHT // 4, WIDTH // 4)) #max pts, i= person, j= key 위치
            # 0: pt_y / (height // 4) * (bb_y2 - bb_y1) + bb_y1
            # 1: pt_x / (width // 4) * (bb_x2 - bb_x1) + bb_x1
            # 2: confidences
            pts[i, j, 0] = pt[0] * 1. / (HEIGHT // 4) * (org.shape[0]) 
            pts[i, j, 1] = pt[1] * 1. / (WIDTH // 4) * (org.shape[1]) 
            #pts[i, j, 0] = pt[0] * 1. / (resolution[0] // 4) * (boxes[i][3] - boxes[i][1]) + boxes[i][1]
            #pts[i, j, 1] = pt[1] * 1. / (resolution[1] // 4) * (boxes[i][2] - boxes[i][0]) + boxes[i][0]
            pts[i, j, 2] = joint[pt]

    ##draw skeleton
    for i, joint in enumerate(joints_dict()['coco']['skeleton']):
        try:
            colors = np.round(
                np.array(plt.get_cmap('tab20').colors) * 255
            ).astype(np.uint8)[:, ::-1].tolist()
        except AttributeError:  # if palette has not pre-defined colors
            colors = np.round(
                np.array(plt.get_cmap('tab20')(np.linspace(0, 1, 16))) * 255
            ).astype(np.uint8)[:, -2::-1].tolist()

        pt1, pt2 = pts[0][joint]

        org = cv2.line(org, (int(pt1[1]), int(pt1[0])), (int(pt2[1]), int(pt2[0])),tuple(colors[0 % len(colors)]), 2)
    ## draw keypoint    
    for idx, P in enumerate(pts):

        circle_size = max(1, min(org.shape[:2]) // 160)
        try:
            colors = np.round(
                np.array(plt.get_cmap('tab20').colors) * 255
            ).astype(np.uint8)[:, ::-1].tolist()
        except AttributeError:  # if palette has not pre-defined colors
            colors = np.round(
                np.array(plt.get_cmap('tab20')(np.linspace(0, 1, 16))) * 255
            ).astype(np.uint8)[:, -2::-1].tolist()

        tag = True
        for k in range(0, 17):
            print(P[k], k)
            key_w= int(P[k][1])# * org.shape[1])
            key_h= int(P[k][0])# * org.shape[0])
            if key_w == 0 and key_h == 0:
                continue
            else:
                frame = cv2.circle(org, (key_w, key_h), circle_size, tuple(colors[k//2 % len(colors)]), -1)
                
    return frame


parser = argparse.ArgumentParser(description='TensorRT pose estimation run')
parser.add_argument('--image', type=str, default='torch2trt/sample.jpg')
args = parser.parse_args()
## Call model path 
MODEL_WEIGHTS = '/opt/nvidia/deepstream/deepstream-5.0/sources/ai_plant/pose/torch2trt/models/pose_hrnet_w32_256x192.pth'
OPTIMIZED_MODEL = '/opt/nvidia/deepstream/deepstream-5.0/sources/ai_plant/pose/torch2trt/models/pose_hrnet_w32_256x192_trt.pth'
model = HRNet(c=32, nof_joints=17).cuda().eval()
WIDTH = 192
HEIGHT = 256
    
## Create trt
if os.path.exists(OPTIMIZED_MODEL) == False:
    data = torch.zeros((1, 3, HEIGHT, WIDTH)).cuda()
    model.load_state_dict(torch.load(MODEL_WEIGHTS))
    model_trt = torch2trt.torch2trt(model, [data], fp16_mode=True)
    torch.save(model_trt.state_dict(), OPTIMIZED_MODEL)
    print('--save_trt')

## Load trt
model_trt = TRTModule()
model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))
print('--load_trt')

## Call IMAGE 
src = cv2.imread(args.image, cv2.IMREAD_COLOR)
orgimg = src.copy()
print(src.shape)
# img = torch.empty((3, HEIGHT, WIDTH))
# img = transform(src[:,:,::-1])

pilimg = execute_2(orgimg)

dir_, filename = os.path.split(args.image)
name, ext = os.path.splitext(filename)
cv2.imwrite('%s_%s.jpg'%('result', name), pilimg)
#pilimg.save('%s_%s.png'%(args.model, name))
```

## torch To tensorrt
2. 코드
```
import torch
from torch2trt import torch2trt
from torch2trt import TRTModule
from models.hrnet import HRNet
import os

def convert_trt(path):

    print("Start converting")
    x = torch.ones((1, 3, 384, 288)).cuda()
    model_trt = torch2trt(model, 
                          [x], 
                          fp16_mode=True,)

    print("Start saving")
    torch.save(model_trt.state_dict(), os.path.join(os.path.dirname(path), os.path.basename(path).split('.pth')[0] + '_trt.pth'))
    
    print("Finish convert")

def LoadExecute_trt(model, path):
    ## tensorRT load
    trt_path = os.path.join(os.path.dirname(path), os.path.basename(path).split('.pth')[0] + '_trt.pth')
    
    model_trt = TRTModule()
    model_trt.load_state_dict(torch.load(trt_path))
    print('Load trt')
                        
    ## tensorRT execute
    # create random data 
    test_data = torch.randn(1, 3, 384, 288).cuda()
                        
    y = model(test_data)
    print('--model ok')
    y_trt = model_trt(test_data)
    print('--trt_model ok')
    print(type(y), type(y_trt))
    
    # check the output against PyTorch
    print(torch.max(torch.abs(y - y_trt)))
    
#hrnet_c 32
checkpoint_path= './torch2trt/models/pose_hrnet_w48_384x288.pth'
device = 'cuda:0'

model = HRNet(c=48, nof_joints=17)
checkpoint = torch.load(checkpoint_path, map_location=device)

if 'model' in checkpoint:
    model.load_state_dict(checkpoint['model'])
else:
    model.load_state_dict(checkpoint)
    
model = model.to(device)

model.eval()

convert_trt(checkpoint_path)

LoadExecute_trt(model, checkpoint_path)
```
