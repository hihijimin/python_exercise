1. 환경
```
torch               1.7.0
torch2trt           0.1.0
torchvision         0.8.0
tensorrt            7.2.1.6
onnx                1.6.0
onnxruntime         1.6.0
```
## torch To onnx
1. check code
```
import torch
from torch2trt import torch2trt
from torch2trt import TRTModule
from models.hrnet import HRNet
import os

import io
import numpy as np
import onnx
import onnxruntime 

def print_size_of_model(model):
    torch.save(model.state_dict(), "temp.pth")
    print('pth Size (MB):', os.path.getsize("temp.pth")/1e6)
    os.remove('temp.pth')

def print_size_of_onnx_model(model):
    onnx.save(model, 'temp.onnx')
    print('onnx Size (MB):', os.path.getsize("temp.onnx")/1e6)
    os.remove('temp.onnx')

def convert_onnx(torch_model,path,batch_size):
    print_size_of_model(torch_model)
    # 모델에 대한 입력값
#     x = torch.randn(batch_size, 3, 256, 192, requires_grad=True).cuda()
    x = torch.randn(batch_size, 3, 256, 192).cuda()
#     torch_out = torch_model(x)
    onnx_save_path = os.path.join(os.path.dirname(path), os.path.basename(path).split('.pth')[0] + '.onnx')
    # 모델 변환
    torch.onnx.export(torch_model,               # 실행될 모델
                      x,                         # 모델 입력값 (튜플 또는 여러 입력값들도 가능)
                      onnx_save_path,            # 모델 저장 경로 (파일 또는 파일과 유사한 객체 모두 가능)
#                       keep_initializers_as_inputs=True,
#                       export_params=True,        # 모델 파일 안에 학습된 모델 가중치를 저장할지의 여부
                      opset_version=11,          # 모델을 변환할 때 사용할 ONNX 버전
#                       do_constant_folding=True,  # 최적하시 constant folding을 사용할지의 여부
                      input_names = ['input'],   # 모델의 입력값을 가리키는 이름
                      output_names = ['output'], # 모델의 출력값을 가리키는 이름
#                       dynamic_axes={'input' : {0 : 'batch_size'},    # 가변적인 길이를 가진 차원
#                                     'output' : {0 : 'batch_size'}})
                     )
    print('--save_onnx')

def LoadExecute_onnx(torch_model,path):
    from onnx import shape_inference 
    # Load onnx
#     onnx_path = os.path.join(os.path.dirname(path), os.path.basename(path).split('.pth')[0] + '.onnx')
#     onnx_path= '/opt/nvidia/deepstream/deepstream-5.0/sources/ai_plant/resnet18_baseline_att_224x224_A_epoch_249.onnx'
    onnx_path= '/opt/nvidia/deepstream/deepstream-5.0/sources/ai_plant/pose_estimation.onnx'
    onnx_model = onnx.load(onnx_path)
    with open('onnx_model_pose_estimation.onnx.txt','w') as f:
        f.write(f"{onnx_model}")
    print('load onnx')
    
    # check the model and shape information
    onnx.checker.check_model(onnx_model)
    print('Before shape inference, the shape info of Y is:\n{}'.format(onnx_model.graph.value_info))
    # apply shape inference on the model
    inferred_model = shape_inference.infer_shapes(onnx_model)
    onnx.checker.check_model(inferred_model)
#     print('After shape inference, the shape info of Y is:\n{}'.format(inferred_model.graph.value_info))
#     with open('infer_model.txt','w') as f:
#         f.write(f"{inferred_model.graph.value_info}")
    
    # get pytorch output
    one_img = torch.randn(1, 3, 256, 192).cuda()
    pytorch_result = torch_model(one_img).cpu().detach().numpy()

    # get onnx output
    input_all = [node.name for node in onnx_model.graph.input]
    input_initializer = [
        node.name for node in onnx_model.graph.initializer
    ]
    net_feed_input = list(set(input_all) - set(input_initializer))
    assert len(net_feed_input) == 1
    # exec onnx 
    sess = onnxruntime.InferenceSession(onnx_path)
    input_name = sess.get_inputs()[0].name
    label_name = sess.get_outputs()[0].name
    print(input_name, label_name)
    onnx_result = sess.run(None,
                           {net_feed_input[0]: one_img.cpu().detach().numpy()
                            })[0]
#     print(onnx_result)

    # only compare part of results - .pth .onnx 오차 확인
    assert np.allclose(
        pytorch_result, onnx_result,
        atol=1.e-5), 'The outputs are different between Pytorch and ONNX'
    print('The numerical values are same between Pytorch and ONNX')
    
    print_size_of_onnx_model(onnx_model)

### 공통 For torch2trt , onnx 
#hrnet_c 32 (channel 수? 마지막단 conv 에서 width?)
checkpoint_path= 'torch2trt/models/pose_hrnet_w32_256x192.pth' # pre_trained 모델 경로

device = 'cuda:0'
batch_size =1 # 임의의 수

model = HRNet(c=32, nof_joints=17) # 정의된 모델 사용하여 모델 생성
checkpoint = torch.load(checkpoint_path, map_location=device)

# 모델을 학습된 가중치로 초기화 
if 'model' in checkpoint:
    model.load_state_dict(checkpoint['model'])
else:
    model.load_state_dict(checkpoint)
    
model = model.to(device)

model.eval() # 모델의 추론모드 (또는 model.train(False) 호출 )
# model.cpu().eval()
## For only onnx
# convert_onnx(model,checkpoint_path, batch_size)
LoadExecute_onnx(model,checkpoint_path)
```
2. inference code
```
import json
#import trt_pose.coco
#import trt_pose.models
import torch
import torch2trt
from torch2trt import TRTModule
import time, sys
import cv2
import torchvision.transforms as transforms
import PIL.Image
#from trt_pose.draw_objects import DrawObjects
#from trt_pose.parse_objects import ParseObjects
import argparse
import os.path

import numpy as np
import os
import sys
import matplotlib.pyplot as plt
#sys.path.append('/opt/nvidia/deepstream/deepstream-5.0/sources/ai_plant/pose/torch2trt')
from models.hrnet import HRNet
print(os.getcwd())
#sys.path.append('/opt/nvidia/deepstream/deepstream-5.0/sources/ai_plant/pose')

def joints_dict():
    joints = {
        "coco": {
            "keypoints": {
                0: "nose",
                1: "left_eye",
                2: "right_eye",
                3: "left_ear",
                4: "right_ear",
                5: "left_shoulder",
                6: "right_shoulder",
                7: "left_elbow",
                8: "right_elbow",
                9: "left_wrist",
                10: "right_wrist",
                11: "left_hip",
                12: "right_hip",
                13: "left_knee",
                14: "right_knee",
                15: "left_ankle",
                16: "right_ankle"
            },
            "skeleton": [
                [15, 13], [13, 11], [16, 14], [14, 12], [11, 12], [5, 11], [6, 12], [5, 6], [5, 7],
                [6, 8], [7, 9], [8, 10], [1, 2], [0, 1], [0, 2], [1, 3], [2, 4],  # [3, 5], [4, 6]
                [0, 5], [0, 6]
            ]
        },
    }
    return joints


def preprocess(image):
    ## Create Image transform
    """
    transform = transforms.Compose([
                    transforms.ToPILImage(), # 이미지 데이터를 PIL image로 바꿔줌
                    transforms.Resize((HEIGHT, WIDTH)), #(256,192)
                    transforms.ToTensor(), # 이미지 데이터를 tensor로 바꿔줌
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # 이미지를 정규화
                ])
    """
    global device
    device = torch.device('cuda')
    
    mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()
    std = torch.Tensor([0.229, 0.224, 0.225]).cuda()
    
    image = cv2.resize(src, dsize=(WIDTH, HEIGHT), interpolation=cv2.INTER_AREA)
    
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = PIL.Image.fromarray(image) # transform 하기 위해선 PIL 형태로 바꿔야 함
    image = transforms.functional.to_tensor(image).to(device)
    image.sub_(mean[:, None, None]).div_(std[:, None, None])
    
    return image[None, ...]

'''
Draw to inference (small)image
'''
def execute(img):
    start = time.time()
    data = preprocess(img)
    cmap, paf = model_trt(data)
    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()
    end = time.time()
    counts, objects, peaks = parse_objects(cmap, paf)#, cmap_threshold=0.15, link_threshold=0.15)
    for i in range(counts[0]):
        print("Human index:%d "%( i ))
        get_keypoint(objects, i, peaks)
    print("Human count:%d len:%d "%(counts[0], len(counts)))
    print('===== Net FPS :%f ====='%( 1 / (end - start)))
    draw_objects(img, counts, objects, peaks)
    return img

'''
Draw to original image
'''
def execute_2(org): 
    print(org.shape)
    data = preprocess(org)
    print(data.shape)
    out = model_trt(data)
    out = out.detach().cpu().numpy() # out = out.detach().cpu()
    print(type(out), out.shape)
    pts = np.empty((out.shape[0], out.shape[1], 3), dtype=np.float32)
    # For each human, for each joint: y, x, confidence
    for i, human in enumerate(out):
        for j, joint in enumerate(human):
            pt = np.unravel_index(np.argmax(joint), (HEIGHT // 4, WIDTH // 4)) #max pts, i= person, j= key 위치
            # 0: pt_y / (height // 4) * (bb_y2 - bb_y1) + bb_y1
            # 1: pt_x / (width // 4) * (bb_x2 - bb_x1) + bb_x1
            # 2: confidences
            pts[i, j, 0] = pt[0] * 1. / (HEIGHT // 4) * (org.shape[0]) 
            pts[i, j, 1] = pt[1] * 1. / (WIDTH // 4) * (org.shape[1]) 
            #pts[i, j, 0] = pt[0] * 1. / (resolution[0] // 4) * (boxes[i][3] - boxes[i][1]) + boxes[i][1]
            #pts[i, j, 1] = pt[1] * 1. / (resolution[1] // 4) * (boxes[i][2] - boxes[i][0]) + boxes[i][0]
            pts[i, j, 2] = joint[pt]

    ##draw skeleton
    for i, joint in enumerate(joints_dict()['coco']['skeleton']):
        try:
            colors = np.round(
                np.array(plt.get_cmap('tab20').colors) * 255
            ).astype(np.uint8)[:, ::-1].tolist()
        except AttributeError:  # if palette has not pre-defined colors
            colors = np.round(
                np.array(plt.get_cmap('tab20')(np.linspace(0, 1, 16))) * 255
            ).astype(np.uint8)[:, -2::-1].tolist()

        pt1, pt2 = pts[0][joint]

        org = cv2.line(org, (int(pt1[1]), int(pt1[0])), (int(pt2[1]), int(pt2[0])),tuple(colors[0 % len(colors)]), 2)
    ## draw keypoint    
    for idx, P in enumerate(pts):

        circle_size = max(1, min(org.shape[:2]) // 160)
        try:
            colors = np.round(
                np.array(plt.get_cmap('tab20').colors) * 255
            ).astype(np.uint8)[:, ::-1].tolist()
        except AttributeError:  # if palette has not pre-defined colors
            colors = np.round(
                np.array(plt.get_cmap('tab20')(np.linspace(0, 1, 16))) * 255
            ).astype(np.uint8)[:, -2::-1].tolist()

        tag = True
        for k in range(0, 17):
            print(P[k], k)
            key_w= int(P[k][1])# * org.shape[1])
            key_h= int(P[k][0])# * org.shape[0])
            if key_w == 0 and key_h == 0:
                continue
            else:
                frame = cv2.circle(org, (key_w, key_h), circle_size, tuple(colors[k//2 % len(colors)]), -1)
                
    return frame


parser = argparse.ArgumentParser(description='TensorRT pose estimation run')
parser.add_argument('--image', type=str, default='torch2trt/sample.jpg')
args = parser.parse_args()
## Call model path 
MODEL_WEIGHTS = '/opt/nvidia/deepstream/deepstream-5.0/sources/ai_plant/pose/torch2trt/models/pose_hrnet_w32_256x192.pth'
OPTIMIZED_MODEL = '/opt/nvidia/deepstream/deepstream-5.0/sources/ai_plant/pose/torch2trt/models/pose_hrnet_w32_256x192_trt.pth'
model = HRNet(c=32, nof_joints=17).cuda().eval()
WIDTH = 192
HEIGHT = 256
    
## Create trt
if os.path.exists(OPTIMIZED_MODEL) == False:
    data = torch.zeros((1, 3, HEIGHT, WIDTH)).cuda()
    model.load_state_dict(torch.load(MODEL_WEIGHTS))
    model_trt = torch2trt.torch2trt(model, [data], fp16_mode=True)
    torch.save(model_trt.state_dict(), OPTIMIZED_MODEL)
    print('--save_trt')

## Load trt
model_trt = TRTModule()
model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))
print('--load_trt')

## Call IMAGE 
src = cv2.imread(args.image, cv2.IMREAD_COLOR)
orgimg = src.copy()
print(src.shape)
# img = torch.empty((3, HEIGHT, WIDTH))
# img = transform(src[:,:,::-1])

pilimg = execute_2(orgimg)

dir_, filename = os.path.split(args.image)
name, ext = os.path.splitext(filename)
cv2.imwrite('%s_%s.jpg'%('result', name), pilimg)
#pilimg.save('%s_%s.png'%(args.model, name))
```

## torch To tensorrt
2. 코드
```
import torch
from torch2trt import torch2trt
from torch2trt import TRTModule
from models.hrnet import HRNet
import os

def convert_trt(path):

    print("Start converting")
    x = torch.ones((1, 3, 384, 288)).cuda()
    model_trt = torch2trt(model, 
                          [x], 
                          fp16_mode=True,)

    print("Start saving")
    torch.save(model_trt.state_dict(), os.path.join(os.path.dirname(path), os.path.basename(path).split('.pth')[0] + '_trt.pth'))
    
    print("Finish convert")

def LoadExecute_trt(model, path):
    ## tensorRT load
    trt_path = os.path.join(os.path.dirname(path), os.path.basename(path).split('.pth')[0] + '_trt.pth')
    
    model_trt = TRTModule()
    model_trt.load_state_dict(torch.load(trt_path))
    print('Load trt')
                        
    ## tensorRT execute
    # create random data 
    test_data = torch.randn(1, 3, 384, 288).cuda()
                        
    y = model(test_data)
    print('--model ok')
    y_trt = model_trt(test_data)
    print('--trt_model ok')
    print(type(y), type(y_trt))
    
    # check the output against PyTorch
    print(torch.max(torch.abs(y - y_trt)))
    
#hrnet_c 32
checkpoint_path= './torch2trt/models/pose_hrnet_w48_384x288.pth'
device = 'cuda:0'

model = HRNet(c=48, nof_joints=17)
checkpoint = torch.load(checkpoint_path, map_location=device)

if 'model' in checkpoint:
    model.load_state_dict(checkpoint['model'])
else:
    model.load_state_dict(checkpoint)
    
model = model.to(device)

model.eval()

convert_trt(checkpoint_path)

LoadExecute_trt(model, checkpoint_path)
```
