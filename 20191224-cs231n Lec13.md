# Inception-v4
![image](https://user-images.githubusercontent.com/56099627/71358432-420e2200-25cc-11ea-8ccc-f5fc58d832d6.png)  
![image](https://user-images.githubusercontent.com/56099627/71358561-bfd22d80-25cc-11ea-9454-8d3113de7c83.png)  
![image](https://user-images.githubusercontent.com/56099627/71358591-d7a9b180-25cc-11ea-98f3-0daa87f5917d.png)  
![image](https://user-images.githubusercontent.com/56099627/71358635-f5771680-25cc-11ea-8b42-9cec2de663bc.png)  
- fully connected layer가 없음. 최종적인 feature map? vector?을 연산하기 위해서 global average을 한다
- valic convolution 사용(padding이 없는 conv사용함)
# Inception-ResNet-v2
![image](https://user-images.githubusercontent.com/56099627/71462890-1d968d80-27f8-11ea-815c-0b3e77b05ef0.png)  
![image](https://user-images.githubusercontent.com/56099627/71462988-5afb1b00-27f8-11ea-9538-1e4789c3fabb.png)  
![image](https://user-images.githubusercontent.com/56099627/71463094-a8778800-27f8-11ea-847c-4bd4c887ccbf.png)  
![image](https://user-images.githubusercontent.com/56099627/71463151-d361dc00-27f8-11ea-9f0d-b745add7f73a.png)  
  
![image](https://user-images.githubusercontent.com/56099627/71463222-01dfb700-27f9-11ea-8145-35c0fe60d199.png)  
- resnet-v2와 v4(non-residual)은 수렴되는 결과는 비슷함. resnet-v2가 v4보다 빠르게 학습 됨
# Segmnetation
### 1. semantic segmentation
- 전통적인 방식의 classification은 one label per image 임
- semantic segmentation은 one label per pixel 임 
 - 하나의 픽셀은 하나의 class으로 인식하므로 4마리의 소를 하나의 cow로 인식하고 각각의 cow으로 인식하진 못함
 - 각 class을 숫자로 부여하고 또 backgraound class을 추가로 두고 부여된 class가 없다면 backgraound class으로 둔다. 

- 모든 픽셀을 레이블링 하고 인스턴스를 인식하지 못한다
![image](https://user-images.githubusercontent.com/56099627/71464451-bfb87480-27fc-11ea-98d0-f1e84a43cc8f.png)  
- 이미지가 있고 특정 부분을 patch 한다. **patch된 부분을 cnn으로 돌리면** 그 patch의 가운데 픽셀이 어떤 것인인지 classify를 하게 됨. 그리고 모든 patch에 대해 반복해서 해줌
  - 사실상 이방법은 비용이 많이 드는 방식임 
- 이미지 전체를 fully convilutional network으로 돌려 모든 픽셀을 한번에 구해낼수 있음. **하지만 중간 과정에서 pooling 또는 stride convolution 등을 통해 다운샘플링 되어 아웃풋이미지가 작아짐**
  
![image](https://user-images.githubusercontent.com/56099627/71466162-16747d00-2802-11ea-8793-acb0b17938d8.png)   
- (확장버전 1) Multi-scale 
 - pyramid segmentation : 다양한 형태의 이미지 scales 추출 - 각 이미지 scale 마다 cnn 구현 - 각기 다른 이미지 scale 을 원본 이미지 크기로 upsale 해줌 - 최종적으로 concatenate 해줌
 - bottom-upsegmentation : (전통적인 방법) super pixels 와 tree 방법 있음, cnn에서 줄수 없는 정보를 큰 맥락에서의 추가적인 정보를 수행한다고 보면 됨
  - super pixels : 근접한 위치에 픽셀을 보고 큰변화가 없으면 연관된 지역으로 묶어주어 영역을 구분해 나감
  - tree : 어떤 픽셀들끼리 merge 되어 나가야 할지 결정해주는 방법
 - 최종적으로 pyramid 와 bottom-up 방법을 묶에 결과를 보임
 
![image](https://user-images.githubusercontent.com/56099627/71466578-59832000-2803-11ea-9cc7-0e16d7af3f2c.png)  
- (확장버전 2) Refinement
 - R,G,B 각각 채널별로 cnn을 적용해서 각각 레이블을 얻어냄 & refinement - 결과 원본보다 다운샘플링된 이미지 생성 & refinement - 이 과정을 반복해줌(이 과정동안 parameter weights을 쉐어링을 해줌. 즉 recurrent neural network(rnn)과 비슷함, 점점 이미지을 나무보단 숲을 보는 듯한 효과줌, 이 반복이 많아질수록 결과는 더 훌륭해짐)
![image](https://user-images.githubusercontent.com/56099627/71467901-92bd8f00-2807-11ea-91bd-a2de9a62aa15.png)    
- (확장버전 3) Upsameling 
- 인풋이미지를 받아서 cnn을 돌려 feature map을 추출해냄(여기까지 과정은 앞의 Multi-scale, Refinement 과 동일) 작아진 feature map을 복원해주기 위해하는 upsampling을 해주는데(업샘플링하는 방법으로 1,2 확장버전들은 하드코딩된 방법들을 이용했는데) updampling 과정도 deep learning network 과정의 일부분으로 편입시킨것으로 다른점이 있음. 그림에서 "Learnable upsampling"이 있음 말 그대로 학습가능한 업샘플링 과정이 있음
![image](https://user-images.githubusercontent.com/56099627/71469015-3bb9b900-280b-11ea-8712-35d0a26d0964.png)   
- fully connected network은 upsampling , skip connection 2가지 특징을 가짐
- 이미지 들어오면 - conv&pool을 거쳐 사이즈가 1/2으로 줄어듦(conv&pool1 - 2배, conv&pool2 - 4배, conv&pool3 - 8배, conv&pool4 - 16배, conv&pool5 - 32배 작아짐) -> Learnable upsampling 을 통해 32배를 크기를 학장시켜줌
- 초기일수록 feature map 은 크고 receptive field은 더 작음 ( 원본이미지의 세세한 부분을 파악하는데 도움되는 곳이 앞쪽 이미지)
- skip connection은  pool5 에선 32배 업샘플링, pool4 에선 16배 업샘플링, pool3 에선 8배 업샘플링한 것을 전체를 combinde 해줘서 결과를 냄
 - (그림결과) skip connection을 가미하면 좀더 좋은 결과를 낼수 있음

![image](https://user-images.githubusercontent.com/56099627/71469430-6ce6b900-280c-11ea-9f7e-697c171b677d.png)  

### 2. instance segmentation
<p align="center"><img width="50%" src="https://user-images.githubusercontent.com/56099627/71469478-a61f2900-280c-11ea-8be5-761423b65cb2.png" /></p> 
  
- 사람 하나의 인스턴스를 detect을 하고 그 각각의 인스턴스 내의 픽셀들을 레이블링(class) 해준다. 이런식으로 인스턴스간 구분을 해줌
- SDS(simultaneous detection and segmentation)이라고 표현하기도 함
![image](https://user-images.githubusercontent.com/56099627/71470191-b0422700-280e-11ea-8d0b-f721d74eedba.png)  
- 이미지로부터 external segment proposals - 각각의 segment proposal 돤 부분에 대해서 feature extraction - box cnn (결과물: bounded box) & region cnn(이 과정에선 mean을 이용해 background을 제거하고 foregraound 만 cnn으로 돌림) - box cnn과 region cnn을concat하여 그 reigon에 대한 classification을 함 
![image](https://user-images.githubusercontent.com/56099627/71470519-c43a5880-280f-11ea-9a53-f4e5ef93462e.png)  
- (확장판 1) hypercolumns 
 - 이미지를 alexnet에 넣고 돌림 그 사이 conv을 진행하는 과정에서 upsampling을 해줌 - 그 upsampling애들을 combind 해준 후 결과보임
 - 각 독립된 픽셀 내에서 로지스틱 classify을 수행하고 결과물에서 각각의 필셀이 얼마나 foregraound/background 되었는지 판단함
  
![image](https://user-images.githubusercontent.com/56099627/71471125-ed5be880-2811-11ea-9f79-f153705aaa43.png)  
- (확장판 2) Cascades
 - faster R-cnn과 유사함
 - 그림에서 모델은 resnet 에 instance segmentation을 얹어서 결과낸 모델임
 - 이미지 자체를 cnn을 시켜 conv feature map을 생성- fast r-cnn에서 사용하는 RPN(Region proposal network)을 사용하여 roi을 뽑아냄 - roi pooing을 이용하여 roi을 동일하게 사이즈를 맞춰주어 fully connected layre에 주어 결과를 냄 - figure/ground logistic regression을 이용해 mask instances을 생성하게 됨- 다시한번 masking 하고 backgreound는 날려버리고 foreground을 가져와 이것을 fully conneected layer에 주어 object의 class를 classification 해줌 
 
<p align="center"><img width="70%" src="https://user-images.githubusercontent.com/56099627/71471250-3e6bdc80-2812-11ea-92a3-e7a8665d3b8d.png" /></p> 
  
# Attention








참고  
[1] http://cs231n.stanford.edu/2016/syllabus.html, (설명) Andu song  
[2] https://www.youtube.com/watch?v=Q9bNbl5FiD8&t=1196s, cs231n 13강 (1) Segmentation  
